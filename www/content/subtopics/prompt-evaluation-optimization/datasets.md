---
title: Public Datasets
weight: 40
description: These datasets provide standardized benchmarks to test and refine prompt engineering approaches.
---
These datasets provide standardized benchmarks to test and refine prompt engineering approaches. The hosts emphasize the need for creativity and experimentation in this evolving field, blending technical expertise with an intuitive understanding of language and machine learning.
<!-- more -->

### **KILT**
KILT (Knowledge Intensive Language Tasks) is a unified benchmark that consolidates 11 datasets across five task types: fact-checking, open-domain question answering, slot filling, entity linking, and dialogue generation. All datasets are aligned with a single knowledge source—a preprocessed snapshot of Wikipedia—allowing for consistent evaluation and enabling multitask and transfer learning. KILT emphasizes not only the accuracy of outputs but also the provenance of the information used, facilitating research into models that justify their predictions with evidence. This benchmark is ideal for developing systems that require real-world knowledge integration and grounding<a href="https://ai.meta.com/blog/introducing-kilt-a-new-unified-benchmark-for-knowledge-intensive-nlp-tasks/" target="_blank">[3]</a><a href="https://ai.meta.com/tools/kilt/" target="_blank">[8]</a>.

### **SuperGLUE**
SuperGLUE is an advanced benchmark for evaluating general-purpose language understanding models. It builds on the original GLUE benchmark by introducing more challenging tasks designed to test reasoning and understanding beyond simple language comprehension. SuperGLUE includes eight primary tasks (e.g., BoolQ, MultiRC, ReCoRD) and two diagnostic tasks, covering areas like Boolean reasoning, multi-sentence reading comprehension, and commonsense reasoning. The focus is on models’ ability to generalize without relying on domain-specific knowledge, making it a comprehensive test for language understanding capabilities<a href="https://klu.ai/glossary/superglue-eval" target="_blank">[4]</a><a href="https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_1.34.50_PM.png?sa=X&ved=2ahUKEwjp9t3Sv5yLAxWd48kDHUhwBtAQ_B16BAgEEAI" target="_blank">[13]</a>.

---

### **Task-Specific Datasets**

#### **Natural Questions (NQ)**
Natural Questions is a question-answering dataset consisting of real user queries from Google Search paired with Wikipedia articles. Annotators provide both long answers (paragraphs) and short answers (specific entities or phrases) when possible. It challenges models to comprehend entire pages of text to locate answers, making it more realistic and complex than earlier QA datasets. This dataset is widely used for training and evaluating open-domain QA systems<a href="https://huggingface.co/datasets/google-research-datasets/natural_questions" target="_blank">[5]</a><a href="https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/" target="_blank">[9]</a><a href="https://www.tensorflow.org/datasets/catalog/natural_questions" target="_blank">[12]</a>.

#### **HotpotQA**
HotpotQA focuses on multi-hop question answering, requiring models to reason across multiple supporting documents to generate answers. It includes 113k question-answer pairs based on Wikipedia, with supporting facts annotated at the sentence level. The dataset also features diverse question types, such as comparison questions and intersectional reasoning tasks. HotpotQA evaluates both answer accuracy (e.g., exact match) and explainability (e.g., identifying supporting facts), making it a key resource for testing complex reasoning in QA systems<a href="https://huggingface.co/datasets/hotpotqa/hotpot_qa" target="_blank">[6]</a><a href="https://paperswithcode.com/dataset/hotpotqa" target="_blank">[10]</a><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/reports/custom/15743318.pdf" target="_blank">[14]</a>.

#### **FEVER**
FEVER (Fact Extraction and Verification) is designed for fact verification tasks. It contains 185k claims generated by altering sentences from Wikipedia, which are labeled as "Supported," "Refuted," or "NotEnoughInfo" based on evidence retrieved from Wikipedia. The dataset encourages models to retrieve relevant evidence and verify claims against it. FEVER is widely used for developing systems that combine information retrieval with reasoning to assess factuality<a href="https://huggingface.co/datasets/fever/feverous" target="_blank">[7]</a><a href="https://huggingface.co/datasets/fever/fever" target="_blank">[11]</a>.

---

### **Applications**
These datasets serve as benchmarks for evaluating various aspects of NLP systems:
- **KILT** supports multitask learning by unifying diverse knowledge-intensive tasks.
- **SuperGLUE** tests general-purpose language understanding.
- Task-specific datasets like NQ, HotpotQA, and FEVER focus on specialized capabilities such as open-domain QA, multi-hop reasoning, and fact verification.

Together, these resources provide standardized environments for benchmarking NLP models across a wide range of tasks while fostering innovation in areas like retrieval-augmented generation (RAG) systems and prompt engineering.

## Citations
- [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14039215/418beb58-3486-4d91-ac33-0570aa9c6645/Evaluating-Large-Language-Models_2_transcribe.txt
- [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/14039215/46e62562-fc09-47ab-9da5-36a84317fd6c/ARES-An-Automated-Evaluation-Framework-for-Retrieval-Augmented-Generation-Systems.pdf
- [3] https://ai.meta.com/blog/introducing-kilt-a-new-unified-benchmark-for-knowledge-intensive-nlp-tasks/
- [4] https://klu.ai/glossary/superglue-eval
- [5] https://huggingface.co/datasets/google-research-datasets/natural_questions
- [6] https://huggingface.co/datasets/hotpotqa/hotpot_qa
- [7] https://huggingface.co/datasets/fever/feverous
- [8] https://ai.meta.com/tools/kilt/
- [9] https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/
- [10] https://paperswithcode.com/dataset/hotpotqa
- [11] https://huggingface.co/datasets/fever/fever
- [12] https://www.tensorflow.org/datasets/catalog/natural_questions
- [14] https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/reports/custom/15743318.pdf
- 
- ---
- Answer from Perplexity: pplx.ai/share