{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b034ca3-f604-4879-b2bf-9097e9095b05",
   "metadata": {},
   "source": [
    "# PySpark Essentials\n",
    "\n",
    "- Spark vs Pandas: Why and when to use Spark\n",
    "- Introduction to Spark DataFrames: Schema, loading data, inspecting DataFrames\n",
    "- Distributed computing basics in Spark: Partitions and transformations\n",
    "- Advanced DataFrame operations:\n",
    "    - Column and row manipulations\n",
    "    - Complex filtering and conditional logic\n",
    "    - GroupBy, aggregations\n",
    "    - Joins and window functions\n",
    "- Handling missing data and data types\n",
    "- Integrating with SQL: Using Spark SQL queries with Python\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb71acb-f739-4df4-b948-18b5b5724c02",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2b164-2814-4c9e-b5e5-c0685754708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    DateType,\n",
    ")\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c440e-7fd2-46d8-ac98-123f799002e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run locally, use four worker nodes. Use this for code development and small problems\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "\n",
    "### Connect to a Spark cluster (e.g. start-connect-server.sh)\n",
    "# SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n",
    "# spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868656e1-d3f9-41af-8d7e-096de6d10cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fb4ee-8d62-46a7-ab28-274dd02a5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165cd68-dcdd-4b93-84c3-f6032b7ff4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0927b88-7686-4621-94ec-44b93b3d4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15d969-3e57-4065-be87-cde28ce7e2d1",
   "metadata": {},
   "source": [
    "## Spark vs Pandas: Why and when to use Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e762b-ebf7-479e-ab6c-a216c8dcba42",
   "metadata": {},
   "source": [
    "## Distributed computing basics in Spark: Partitions and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373f2d2-d041-457a-93b4-a5a72af7da05",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "![](northwind-schema.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe210c-5666-4f95-8c6e-c9b27c06de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f40cd-ea8c-427e-b19f-e9d801d035ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "\n",
    "# Method 1: Auto-inference (simple but slower)\n",
    "print(\"=== Method 1: Auto-inference ===\")\n",
    "categories_auto = spark.read.csv(f\"{data_path}categories.csv\", header=True, inferSchema=True)\n",
    "categories_auto.printSchema()\n",
    "categories_auto.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad545fb7-6ccc-4c57-858c-e78fb30201d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Define schema (faster and more control)\n",
    "print(\"\\n=== Method 2: Predefined Schema ===\")\n",
    "\n",
    "# Categories schema\n",
    "categories_schema = StructType([\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"CategoryName\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Picture\", StringType(), True)\n",
    "])\n",
    "\n",
    "categories_schema_df = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "categories_schema_df.printSchema()\n",
    "categories_schema_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc6f61-9463-4fc8-938b-6c0e59eea97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Load as Pandas Dataframe \n",
    "print(\"\\n=== Method 3: Load as Pandas Dataframe  ===\")\n",
    "\n",
    "cat_df = pd.read_csv(f\"{data_path}categories.csv\")\n",
    "categories_pandas_df = spark.createDataFrame(cat_df)\n",
    "categories_pandas_df.printSchema()\n",
    "categories_pandas_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018805a-7630-40b2-8577-e2fe88d7f858",
   "metadata": {},
   "source": [
    "### Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191ebeb-212e-47a8-8aca-ba3a89b805f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories schema\n",
    "categories_schema = StructType([\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"CategoryName\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Picture\", StringType(), True)\n",
    "])\n",
    "\n",
    "# categories_schema_df = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "# categories_schema_df.printSchema()\n",
    "# categories_schema_df.show(5)\n",
    "\n",
    "# Customers schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CompanyName\", StringType(), True),\n",
    "    StructField(\"ContactName\", StringType(), True),\n",
    "    StructField(\"ContactTitle\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"PostalCode\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Phone\", StringType(), True),\n",
    "    StructField(\"Fax\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Orders schema\n",
    "orders_schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"RequiredDate\", DateType(), True),\n",
    "    StructField(\"ShippedDate\", DateType(), True),\n",
    "    StructField(\"ShipVia\", IntegerType(), True),\n",
    "    StructField(\"Freight\", FloatType(), True),\n",
    "    StructField(\"ShipName\", StringType(), True),\n",
    "    StructField(\"ShipAddress\", StringType(), True),\n",
    "    StructField(\"ShipCity\", StringType(), True),\n",
    "    StructField(\"ShipRegion\", StringType(), True),\n",
    "    StructField(\"ShipPostalCode\", StringType(), True),\n",
    "    StructField(\"ShipCountry\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Products schema\n",
    "products_schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"SupplierID\", IntegerType(), True),\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"QuantityPerUnit\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"UnitsInStock\", IntegerType(), True),\n",
    "    StructField(\"UnitsOnOrder\", IntegerType(), True),\n",
    "    StructField(\"ReorderLevel\", IntegerType(), True),\n",
    "    StructField(\"Discontinued\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Employees schema\n",
    "employees_schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"LastName\", StringType(), True),\n",
    "    StructField(\"FirstName\", StringType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"TitleOfCourtesy\", StringType(), True),\n",
    "    StructField(\"BirthDate\", DateType(), True),\n",
    "    StructField(\"HireDate\", DateType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"PostalCode\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"HomePhone\", StringType(), True),\n",
    "    StructField(\"Extension\", StringType(), True),\n",
    "    StructField(\"Photo\", StringType(), True),\n",
    "    StructField(\"Notes\", StringType(), True),\n",
    "    StructField(\"ReportsTo\", IntegerType(), True),\n",
    "    StructField(\"PhotoPath\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d1291-a45c-45d2-9306-5f7f9a3fbe35",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555df5-978c-43ea-bf16-0adf345f04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables with schemas\n",
    "print(\"\\n=== Loading all tables with schemas ===\")\n",
    "categories = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "customers = spark.read.csv(f\"{data_path}customers.csv\", header=True, schema=customers_schema)\n",
    "orders = spark.read.csv(f\"{data_path}orders.csv\", header=True, schema=orders_schema)\n",
    "products = spark.read.csv(f\"{data_path}products.csv\", header=True, schema=products_schema)\n",
    "employees = spark.read.csv(f\"{data_path}employees.csv\", header=True, schema=employees_schema)\n",
    "\n",
    "\n",
    "print(\"Categories:\")\n",
    "categories.printSchema()\n",
    "print(f\"Count: {categories.count()}\")\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers.printSchema()\n",
    "print(f\"Count: {customers.count()}\")\n",
    "\n",
    "print(\"\\nOrders:\")\n",
    "orders.printSchema()\n",
    "print(f\"Count: {orders.count()}\")\n",
    "\n",
    "print(\"\\nProducts:\")\n",
    "products.printSchema()\n",
    "print(f\"Count: {products.count()}\")\n",
    "\n",
    "print(\"\\Employees:\")\n",
    "products.printSchema()\n",
    "print(f\"Count: {employees.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe17a5-dd85-45d5-a449-d6d96bac0a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372de6e8-c1f7-4771-92ee-946e7d905eeb",
   "metadata": {},
   "source": [
    "## Advanced DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf9540-60a3-48a1-9d34-15dbea88d220",
   "metadata": {},
   "source": [
    "### Column and row manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3386758-e318-4a5d-a600-6f945de2875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, upper, concat, lit, year, month, round, desc\n",
    "# Alternatively...\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e3abd-f5b4-4b43-b417-29771eb100d4",
   "metadata": {},
   "source": [
    "Select Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1c7a3-0d41-4fac-9f1a-03ac2011fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only customer ID, company name, and country\n",
    "customers.select(\"CustomerID\", \"CompanyName\", \"Country\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddc074-f38c-49bd-a664-c689ef34b953",
   "metadata": {},
   "source": [
    "Add New Column with Literal Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034da1b5-34a1-4230-a262-bad72e166ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant 'Active' status column to all customers\n",
    "customers_with_status = customers.withColumn(\"Status\", lit(\"Active\"))\n",
    "customers_with_status.select(\"CustomerID\", \"CompanyName\", \"Status\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4e6fa-5e78-42f8-af47-cc21a5adec67",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7b129-4698-43e3-bbb6-58885dd36aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename CustomerID to ID and CompanyName to Company\n",
    "customers_renamed = customers.withColumnRenamed(\"CustomerID\", \"ID\").withColumnRenamed(\"CompanyName\", \"Company\")\n",
    "customers_renamed.select(\"ID\", \"Company\", \"Country\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0f723-5bd0-4501-a6d4-c62b1a3a1813",
   "metadata": {},
   "source": [
    "Conditional Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f6ed9-4b42-4257-be83-0ad2d1baa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize products as 'Expensive' if price > 20, else 'Affordable'\n",
    "products_categorized = products.withColumn(\"PriceCategory\", \n",
    "    when(col(\"UnitPrice\") > 20, \"Expensive\").otherwise(\"Affordable\"))\n",
    "products_categorized.select(\"ProductName\", \"UnitPrice\", \"PriceCategory\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013eb338-7fef-486e-ac70-ab4f73f8a27a",
   "metadata": {},
   "source": [
    "String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9c57d-b9f3-43b3-9174-6db4bbf70e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert company names to uppercase and create full address\n",
    "customers_formatted = customers.withColumn(\"CompanyUpper\", upper(col(\"CompanyName\"))) \\\n",
    "    .withColumn(\"FullAddress\", concat(col(\"Address\"), lit(\", \"), col(\"City\"), lit(\", \"), col(\"Country\")))\n",
    "customers_formatted.select(\"CompanyUpper\", \"FullAddress\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411eb3d8-992e-4b34-afb2-bb153d3694be",
   "metadata": {},
   "source": [
    "Filter Rows Based on Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9e926-630a-4af8-a9b3-23e5547aef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only customers from USA or Germany\n",
    "filtered_customers = customers.filter((col(\"Country\") == \"USA\") | (col(\"Country\") == \"Germany\"))\n",
    "filtered_customers.select(\"CompanyName\", \"Country\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f762640-68e8-4535-a90c-bdc1b1614b69",
   "metadata": {},
   "source": [
    "Extract Date Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798c89b-1741-454a-b46a-dc5f86efbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from order dates\n",
    "orders_with_date_parts = orders.withColumn(\"OrderYear\", year(col(\"OrderDate\"))) \\\n",
    "    .withColumn(\"OrderMonth\", month(col(\"OrderDate\")))\n",
    "orders_with_date_parts.select(\"OrderID\", \"OrderDate\", \"OrderYear\", \"OrderMonth\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a8e1b-11ca-4387-8ba6-c17fb0161d3c",
   "metadata": {},
   "source": [
    "Round Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c82e85-0924-4137-94d2-2539eb73b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round freight costs to 2 decimal places and create freight categories\n",
    "orders_rounded = orders.withColumn(\"FreightRounded\", round(col(\"Freight\"), 2)) \\\n",
    "    .withColumn(\"FreightCategory\", \n",
    "        when(col(\"Freight\") < 10, \"Low\")\n",
    "        .when(col(\"Freight\") < 50, \"Medium\")\n",
    "        .otherwise(\"High\"))\n",
    "orders_rounded.select(\"OrderID\", \"Freight\", \"FreightRounded\", \"FreightCategory\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd8880-f1cc-4ed8-a68e-57ffd1dbea14",
   "metadata": {},
   "source": [
    "Sort and Limit Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af1209-67ab-48f4-b3e9-4bae5804efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 most expensive products, sorted by price descending\n",
    "expensive_products = products.orderBy(desc(\"UnitPrice\")).limit(5)\n",
    "expensive_products.select(\"ProductName\", \"UnitPrice\", \"CategoryID\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d64e3-d948-49da-9c05-5d31f915abb4",
   "metadata": {},
   "source": [
    "### Complex filtering and conditional logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45549bf7-7fb3-4254-9416-e37e02ec470c",
   "metadata": {},
   "source": [
    "**Multiple AND/OR Conditions with Null Handling**\n",
    "\n",
    "This combines multiple conditions using AND (&) and OR (|) operators. It finds customers who either: (1) are from specific European countries AND have both phone and fax numbers, OR (2) are from the USA. The isNotNull() function handles missing data gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481489e-3be0-4e34-901e-dd2edfa5f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segments = customers.withColumn(\"Segment\",\n",
    "    when(col(\"Country\") == \"USA\", \n",
    "         when(col(\"Region\").isNotNull(), \"US-Regional\")\n",
    "         .otherwise(\"US-National\"))\n",
    "    .when(col(\"Country\").isin(\"Germany\", \"France\", \"UK\"), \"EU-Premium\")\n",
    "    .when(col(\"Country\").isin(\"Brazil\", \"Argentina\", \"Mexico\"), \"LATAM\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "customer_segments.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360b3b1-2af6-424d-ac26-29b5046c872b",
   "metadata": {},
   "source": [
    "**Nested Conditional Logic (CASE-WHEN)**\n",
    "\n",
    "This creates nested conditional logic similar to SQL's CASE-WHEN. It segments customers into tiers based on country and region, with nested conditions for US customers (regional vs national) and different categories for other geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd6a67-a661-46bd-bf73-ecb921c3bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segments = customers.withColumn(\"Segment\",\n",
    "    when(col(\"Country\") == \"USA\", \n",
    "         when(col(\"Region\").isNotNull(), \"US-Regional\")\n",
    "         .otherwise(\"US-National\"))\n",
    "    .when(col(\"Country\").isin(\"Germany\", \"France\", \"UK\"), \"EU-Premium\")\n",
    "    .when(col(\"Country\").isin(\"Brazil\", \"Argentina\", \"Mexico\"), \"LATAM\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "customer_segments.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465dcee-ebef-4482-bfa7-549f57235fb4",
   "metadata": {},
   "source": [
    "**Date-Based Filtering with Calculations**\n",
    "\n",
    "This identifies problematic orders using date calculations. It finds orders that are: (1) shipped more than 7 days late, (2) have high freight costs for international shipments, or (3) remain unshipped after 30 days. The datediff() function calculates differences between dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b69ae-10a7-4973-a438-2f99540ed3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "problematic_orders = orders.filter(\n",
    "    (datediff(col(\"ShippedDate\"), col(\"RequiredDate\")) > 7) |\n",
    "    ((col(\"Freight\") > 100) & (col(\"ShipCountry\") != \"USA\")) |\n",
    "    (col(\"ShippedDate\").isNull() & (datediff(current_date(), col(\"OrderDate\")) > 30))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee82db-aaa2-4a5b-a02b-0007a1166e46",
   "metadata": {},
   "source": [
    "**String Pattern Matching with Conditional Transformations**\n",
    "\n",
    "This uses regular expressions to categorize products based on name patterns. The `(?i)` flag makes the search case-insensitive, and the pipe `(|)` acts as OR within the regex. It then applies different pricing strategies based on the categorization and other business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22057c-a10b-469d-9595-69743d0a6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "product_categories = products.withColumn(\"ProductCategory\",\n",
    "    when(col(\"ProductName\").rlike(\"(?i).*(cheese|dairy).*\"), \"Dairy\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(wine|beer|ale).*\"), \"Alcoholic\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(sauce|syrup|spread).*\"), \"Condiments\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(tea|coffee).*\"), \"Beverages\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898c028-6bc5-4feb-863e-81c91f3bb91d",
   "metadata": {},
   "source": [
    "**Complex Inventory Management Logic**\n",
    "\n",
    "This implements complex business logic for inventory management. It creates alerts for different scenarios: out-of-stock active products, low stock below reorder levels, expensive overstocked items, and products that need reordering. The final filter removes \"Normal\" items to focus only on products requiring attention.\n",
    "\n",
    "The .rlike() method performs regex matching on column values. The pattern (?i).*(cheese|dairy).* means:\n",
    "\n",
    "- `(?i)` - Case insensitive flag\n",
    "- `.*` - Match any characters before\n",
    "- `(cheese|dairy)` - Match either \"cheese\" or \"dairy\"\n",
    "- `.*` - Match any characters after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be88065-324a-4348-b8f8-9d36b8db2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_alerts = products.withColumn(\"AlertType\",\n",
    "    when((col(\"UnitsInStock\") == 0) & (col(\"Discontinued\") == 0), \"Out of Stock\")\n",
    "    .when((col(\"UnitsInStock\") < col(\"ReorderLevel\")) & (col(\"Discontinued\") == 0), \"Low Stock\")\n",
    "    .when((col(\"UnitsInStock\") > 100) & (col(\"UnitPrice\") > 20), \"Overstock Expensive\")\n",
    "    .when((col(\"UnitsOnOrder\") == 0) & (col(\"UnitsInStock\") < 20) & (col(\"Discontinued\") == 0), \"No Reorder\")\n",
    "    .otherwise(\"Normal\")\n",
    ").filter(col(\"AlertType\") != \"Normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7bad6-d120-4c4e-af5c-36838a3c559c",
   "metadata": {},
   "source": [
    "### Joins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6e873-c2e4-4c52-9dd9-76bd86b95daf",
   "metadata": {},
   "source": [
    "**Inner Join - Basic Relationship**\n",
    "\n",
    "This performs a standard inner join between orders and customers tables on the CustomerID column. Only orders that have matching customers are returned. This is the most common join type and provides the best performance since it filters out non-matching records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f78b6-5a19-4587-84b7-68d034563c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_customers = orders.join(customers, \"CustomerID\", \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b681-0d57-4754-a62a-d48d6117a721",
   "metadata": {},
   "source": [
    "**Left Join - Preserve All Records**\n",
    "\n",
    "A left join preserves all customers even if they have no orders. The aggregation counts orders per customer, showing 0 for customers without orders. This is useful for finding inactive customers or getting complete customer metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bb499-85d8-4c9f-b64c-bac434fb05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "customer_orders = customers.join(orders, \"CustomerID\", \"left\") \\\n",
    "    .groupBy(\"CustomerID\", \"CompanyName\") \\\n",
    "    .agg(count(\"OrderID\").alias(\"OrderCount\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fc87c-a9d6-4b92-9675-6ce4f4e4c9cb",
   "metadata": {},
   "source": [
    "**Broadcast Join - Small Table Optimization**\n",
    "\n",
    "The `broadcast()` function tells Spark to send the small categories table to all worker nodes, avoiding expensive shuffle operations. This is highly efficient when one table is small (<200MB). Spark automatically broadcasts tables under 10MB, but explicit broadcasting ensures optimization for larger small tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d5c0c-420c-457f-8009-50bb69659f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "products_categories = products.join(broadcast(categories), \"CategoryID\", \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276bc5-73a4-4aa7-8f33-cced19d00430",
   "metadata": {},
   "source": [
    "**Multiple Table Join - Complex Business Query**\n",
    "\n",
    "This chains multiple joins to create a comprehensive view. Note the explicit column reference `(orders.EmployeeID == employees.EmployeeID)` when column names might be ambiguous. This pattern is common in data warehousing for creating denormalized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1863ffa-221d-407c-8eb5-f62b3d780bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_orders = orders \\\n",
    "    .join(customers, \"CustomerID\", \"inner\") \\\n",
    "    .join(employees, orders.EmployeeID == employees.EmployeeID, \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2c97d-3a11-44e9-bacd-d87140e3bf2f",
   "metadata": {},
   "source": [
    "**Inequality Join - Range-Based Matching**\n",
    "\n",
    "Unlike equality joins, this uses range conditions to match products to price tiers. The join condition `(UnitPrice >= MinPrice) & (UnitPrice < MaxPrice)` assigns each product to its appropri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eccea9-ca4f-47fd-b9c6-cab0108a7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price tiers DataFrame\n",
    "price_tiers = spark.createDataFrame([\n",
    "    (1, \"Budget\", 0.0, 10.0),\n",
    "    (2, \"Standard\", 10.0, 25.0),\n",
    "    (3, \"Premium\", 25.0, 50.0),\n",
    "    (4, \"Luxury\", 50.0, 999.0)\n",
    "], [\"TierID\", \"TierName\", \"MinPrice\", \"MaxPrice\"])\n",
    "\n",
    "products_tiers = products.join(\n",
    "    price_tiers,\n",
    "    (col(\"UnitPrice\") >= col(\"MinPrice\")) & (col(\"UnitPrice\") < col(\"MaxPrice\")),\n",
    "    \"inner\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e636795-062b-4df1-8a29-35f3f44183e6",
   "metadata": {},
   "source": [
    "#### Attributes vs Col\n",
    "\n",
    "This FAILS - no schema defined\n",
    "    df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")])\n",
    "    result = df.CustomerID  # AttributeError: 'DataFrame' object has no attribute 'CustomerID'\n",
    "\n",
    "\n",
    "This WORKS - schema defined\n",
    "    schema = StructType([\n",
    "        StructField(\"CustomerID\", IntegerType(), True),\n",
    "        StructField(\"Name\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], schema)\n",
    "    result = df.CustomerID  # Works!\n",
    "\n",
    "Why This Happens\n",
    "- Python's Dynamic Nature: Python resolves attributes at runtime using __getattr__\n",
    "- Unknown Column Names: Without schema, Spark doesn't know what columns exist\n",
    "- Lazy Evaluation: Schema inference happens only when an action is triggered\n",
    "\n",
    "Solutions\n",
    "- Use `col()` function (always works):\n",
    "        ```df.select(col(\"CustomerID\"))  # Works with or without schema```\n",
    "- Use bracket notation:\n",
    "        ```df[\"CustomerID\"]  # Works with or without schema```\n",
    "- Define schema explicitly:\n",
    "        ```df = spark.createDataFrame(data, schema)\n",
    "        df.CustomerID  # Now works```\n",
    "\n",
    "The attribute notation (df.ColumnName) is syntactic sugar that only works when Spark knows the column exists at DataFrame creation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0757c1-a1d8-47ce-85c4-3e0c0f138c21",
   "metadata": {},
   "source": [
    "### GroupBy, aggregations and window functions\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- Grouping: Collapses rows into groups for aggregation\n",
    "- Window Functions: Perform calculations across related rows without collapsing\n",
    "- Partitioning: Divides data into logical groups for separate processing\n",
    "- Ordering: Determines sequence for ranking and frame-based operations\n",
    "- Frames: Define which rows to include in window calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d2176-c1b1-48c8-8aa9-dfc6ce060564",
   "metadata": {},
   "source": [
    " **Basic Grouping and Aggregation**\n",
    "\n",
    "Groups orders by shipping country and calculates basic statistics. The agg() function allows multiple aggregations in one operation, providing count, average, and sum of freight costs per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec03493-2606-4c2c-8c18-c052b2f60a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum # naming conflict when using other package\n",
    "# from pyspark.sql.functions import sum as spark_sum ...or use F.sum()\n",
    "\n",
    "country_stats = orders.groupBy(\"ShipCountry\") \\\n",
    "    .agg(\n",
    "        count(\"OrderID\").alias(\"OrderCount\"),\n",
    "        avg(\"Freight\").alias(\"AvgFreight\"),\n",
    "        sum(\"Freight\").alias(\"TotalFreight\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739d33c-fc77-4413-950f-2dd9fdde8efd",
   "metadata": {},
   "source": [
    "**Multiple Column Grouping**\n",
    "\n",
    "Groups by multiple columns (category and supplier) to create a cross-tabulation analysis. This shows how products are distributed across category-supplier combinations with their pricing and inventory metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901abad4-7932-4997-8331-46ff0d599663",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_supplier_stats = products.groupBy(\"CategoryID\", \"SupplierID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"ProductCount\"),\n",
    "        avg(\"UnitPrice\").alias(\"AvgPrice\"),\n",
    "        sum(\"UnitsInStock\").alias(\"TotalStock\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774a389-6240-4555-859a-bd89fbffb41b",
   "metadata": {},
   "source": [
    "**Window Function - Ranking**\n",
    "\n",
    "Creates cumulative sums using window frames. `rowsBetween(unboundedPreceding, currentRow)` includes all rows from the start of the partition to the current row, creating a running total of freight costs per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6cb87-6be2-4280-9ee9-441b2b464d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "customer_window = Window.partitionBy(\"CustomerID\").orderBy(\"OrderDate\")\n",
    "running_totals = orders.withColumn(\"RunningFreight\", \n",
    "    sum(\"Freight\").over(customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd6e2e-6d3b-45e5-978e-50b9dd7a7284",
   "metadata": {},
   "source": [
    "**Window Function - Lag and Lead**\n",
    "\n",
    "`lag()` and `lead()` access previous and next rows within a partition. This enables comparison between consecutive orders for the same customer, useful for trend analysis and change detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cdae4-0b3d-45b6-91b1-cdd9ae43e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "comparison_window = Window.partitionBy(\"CustomerID\").orderBy(\"OrderDate\")\n",
    "order_comparison = orders.withColumn(\"PrevFreight\", lag(\"Freight\", 1).over(comparison_window))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43aafc-6963-4f49-8f89-be5b5624a0da",
   "metadata": {},
   "source": [
    "**Advanced Aggregation with Conditional Logic**\n",
    "\n",
    "Combines aggregation with conditional logic using `when()`. This counts total products and conditionally counts discontinued products, enabling calculation of percentages and business metrics within the same aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aa569-ed4c-44ef-9105-8d63422c2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_performance = products.groupBy(\"CategoryID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"TotalProducts\"),\n",
    "        count(when(col(\"Discontinued\") == 1, col(\"ProductID\"))).alias(\"DiscontinuedCount\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0f4c8-b96e-4d29-b4e4-c318209c7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "product_performance = products.groupBy(\"CategoryID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"TotalProducts\"),\n",
    "        sum(when(col(\"Discontinued\") == 1, 1).otherwise(0)).alias(\"DiscontinuedCount\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c7271-5826-4f3d-b305-8faeac0e3579",
   "metadata": {},
   "source": [
    "**Window Function - Statistical Measures**\n",
    "\n",
    " Applies statistical functions across window partitions without collapsing rows (unlike `groupBy`). Each product retains its row while gaining category-level statistics, enabling comparison of individual items against group averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9cf2-799d-4059-a760-ff3494a4527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_window = Window.partitionBy(\"CategoryID\")\n",
    "product_stats = products.withColumn(\"AvgCategoryPrice\", avg(\"UnitPrice\").over(stats_window))\n",
    "product_stats.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa78d9-4be1-4256-9069-f01f3d69de88",
   "metadata": {},
   "source": [
    "## Data Types ad Handling Missing Data \n",
    "\n",
    "Spark DataFrames support a rich type system that mirrors SQL data types while providing strong schema enforcement and optimization capabilities. The core data types include\n",
    "- primitive types (IntegerType, StringType, FloatType, DoubleType, BooleanType, DateType, TimestampType),\n",
    "- decimal types (DecimalType for precise numeric calculations), and\n",
    "- complex types (ArrayType, MapType, StructType for nested data).\n",
    "\n",
    "Each column in a DataFrame has a defined data type that determines how Spark stores, processes, and optimizes operations on that data. Unlike Python's dynamic typing, Spark's static typing enables the Catalyst optimizer to generate efficient execution plans and catch type-related errors at compile time rather than runtime.\n",
    "\n",
    "The nullable property in Spark schemas plays a crucial role in data integrity and query optimization. When a column is marked as `nullable=False`, Spark guarantees that the column cannot contain null values, enabling aggressive optimizations like eliminating null checks in generated code.\n",
    "\n",
    "Conversely, `nullable=True` columns require null-safe operations and additional runtime checks. The `NullType` represents columns that contain only null values, which can occur during data loading or as intermediate results in transformations. \n",
    "\n",
    "Proper null handling is essential because null values propagate through most operations (e.g., `null + 5 = null`) and require explicit handling using functions like `isNull()`, `isNotNull()`, `coalesce()`, or `when().otherwise()` constructs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b0276-8bd1-4fc1-9d9b-1916ddafd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema definition with nullable control\n",
    "StructField(\"CustomerID\", StringType(), False)     # Cannot be null\n",
    "StructField(\"Region\", StringType(), True)          # Can be null\n",
    "\n",
    "# Null value analysis\n",
    "customers.select([count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") for c in customers.columns])\n",
    "\n",
    "# Null-safe operations\n",
    "when(col(\"Region\").isNull(), \"No Region\").otherwise(col(\"Region\"))\n",
    "\n",
    "# Type conversions\n",
    "col(\"UnitPrice\").cast(IntegerType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97196fbd-3cfc-4882-9933-fac96c525561",
   "metadata": {},
   "source": [
    "PySpark provides comprehensive tools for handling missing data through its DataFrame API and DataFrameNaFunctions class. Missing data in Spark is primarily represented as NULL values, which are distinct from empty strings, zeros, or NaN (Not a Number) values.\n",
    "\n",
    "Spark's approach to missing data is SQL-compliant, meaning NULL values propagate through most operations (e.g., NULL + 5 = NULL) and are excluded from aggregations by default. The framework offers both automatic handling (like ignoring NULLs in aggregations) and explicit control through functions like `isNull()`, `isNotNull()`, `na.drop()`, and `na.fill()`.\n",
    "\n",
    "The DataFrameNaFunctions class, accessed via df.na, provides the primary interface for missing data operations. Key strategies include\n",
    "- dropping rows with na.drop() (with options for thresholds and specific columns),\n",
    "- filling values with na.fill() (supporting different values per column), and\n",
    "- replacing values with na.replace().\n",
    "\n",
    "Advanced techniques involve conditional imputation using\n",
    "- `when().otherwise()` constructs,\n",
    "-  forward/backward filling with window functions, and\n",
    "-   statistical imputation using calculated means or medians.\n",
    "\n",
    "Spark also provides null-safe operations like `eqNullSafe()` for comparisons and coalesce() for selecting the first non-null value from multiple columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330b54c-f2c1-43c7-93e8-cf5d108445fd",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDFs) and User Defined Aggregate Functions (UDAFs)\n",
    "\n",
    "**User Defined Functions (UDFs)** in PySpark allow you to extend Spark's built-in function library with custom Python logic that can be applied to DataFrame columns. UDFs are essential when built-in functions cannot handle specific business logic, complex string processing, or domain-specific calculations. \n",
    "\n",
    "There are two main types:\n",
    "1. Standard UDFs that process one row at a time, and\n",
    "2. Pandas UDFs (vectorized UDFs) that leverage Apache Arrow for better performance by processing entire columns as pandas Series.\n",
    "\n",
    "UDFs are registered using the `udf()` function with a specified return type, and they can handle complex operations like regex processing, mathematical calculations, or external API calls.\n",
    "\n",
    "**User Defined Aggregate Functions (UDAFs)** enable custom aggregation logic across groups of rows, though PySpark doesn't have direct UDAF support like Scala Spark. Instead, you can achieve similar functionality by combining `collect_list()` with UDFs, or using `pandas_udf` with `groupby().apply()` for more complex aggregations. These are useful for implementing custom statistical measures, weighted calculations, or business-specific metrics that aren't available in Spark's standard aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfe3bc-0b85-4253-ae3d-3ca5e0b96ab3",
   "metadata": {},
   "source": [
    "**Standard UDF - Phone Number Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93541331-735b-447b-aa0c-4c3e8071bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def standardize_phone(phone_str):\n",
    "    if not phone_str:\n",
    "        return None\n",
    "    digits = re.sub(r'\\D', '', phone_str)\n",
    "    if len(digits) == 10:\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "    # ... complex formatting logic\n",
    "    \n",
    "standardize_phone_udf = udf(standardize_phone, StringType())\n",
    "customers.withColumn(\"StandardizedPhone\", standardize_phone_udf(col(\"Phone\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a70e6-98ef-4bbe-92fb-a5705b0d688f",
   "metadata": {},
   "source": [
    "**Custom Aggregation - Weighted Average**\n",
    "\n",
    "Performance Considerations: Pandas UDFs are significantly faster than standard UDFs due to vectorization and reduced serialization overhead. However, UDFs should be used judiciously as they break Spark's optimization capabilities and require data movement between JVM and Python processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c391dd9-4a73-4325-b99f-fe4cc5c717e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "def weighted_average_freight(freight_values, order_counts):\n",
    "    total_weighted = sum(f * c for f, c in zip(freight_values, order_counts) if f and c)\n",
    "    total_weights = sum(c for c in order_counts if c)\n",
    "    return total_weighted / total_weights if total_weights > 0 else 0.0\n",
    "\n",
    "orders.groupBy(\"ShipCountry\").agg(collect_list(\"Freight\").alias(\"freight_list\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fe31a-a7ad-4def-a37d-7a6c8a5b14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0c8b3-844d-464c-a456-505b5ae204da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4607db93-ef76-4a90-80f8-4561309a6e73",
   "metadata": {},
   "source": [
    "**Pandas UDF - Vectorized Z-Score Calculation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bf91e-6e9f-4062-af4b-b2620f0f5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, col\n",
    "# from pyspark.sql.types import StructType, StructField, FloatType\n",
    "# import pandas as pd\n",
    "\n",
    "# def calculate_zscore_group(pdf):\n",
    "#     \"\"\"Calculate Z-score within each group\"\"\"\n",
    "#     mean_price = pdf['UnitPrice'].mean()\n",
    "#     std_price = pdf['UnitPrice'].std()\n",
    "    \n",
    "#     if std_price > 0:\n",
    "#         pdf['PriceZScore'] = (pdf['UnitPrice'] - mean_price) / std_price\n",
    "#     else:\n",
    "#         pdf['PriceZScore'] = 0.0\n",
    "    \n",
    "#     return pdf2\n",
    "\n",
    "# # Create the output schema by adding the new column\n",
    "# # output_schema = StructType(products.schema.fields + [StructField(\"PriceZScore\", FloatType(), True)])\n",
    "# schema_str = \"ProductID int, ProductName string, SupplierID int, CategoryID int, QuantityPerUnit string, UnitPrice float, UnitsInStock int, UnitsOnOrder int, ReorderLevel int, Discontinued int, PriceZScore float\"\n",
    "\n",
    "# products_zscore = products.groupBy(\"CategoryID\").applyInPandas(\n",
    "#     calculate_zscore_group, \n",
    "#     schema=schema_str\n",
    "# )\n",
    "\n",
    "# # # Apply to grouped data\n",
    "# # products_zscore = products.groupBy(\"CategoryID\").applyInPandas(\n",
    "# #     calculate_zscore_group, \n",
    "# #     schema=output_schema\n",
    "# # )\n",
    "# products_zscore.printSchema()\n",
    "\n",
    "# #products_zscore.select(\"CategoryID\", \"ProductName\", \"UnitPrice\", \"PriceZScore\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468a0dc-b024-4d61-a798-e4bb03a6909a",
   "metadata": {},
   "source": [
    "Alternatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2084d-6161-464d-97ad-953c387d34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window\n",
    "window = Window.partitionBy(\"CategoryID\")\n",
    "\n",
    "# Calculate Z-score using window functions\n",
    "products_zscore = products \\\n",
    "    .withColumn(\"AvgPrice\", avg(\"UnitPrice\").over(window)) \\\n",
    "    .withColumn(\"StdPrice\", stddev(\"UnitPrice\").over(window)) \\\n",
    "    .withColumn(\"PriceZScore\", \n",
    "        (col(\"UnitPrice\") - col(\"AvgPrice\")) / col(\"StdPrice\")\n",
    "    )\n",
    "\n",
    "# Now you can select the column\n",
    "products_zscore.select(\"CategoryID\", \"ProductName\", \"UnitPrice\", \"PriceZScore\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076c7b-52ed-4cb6-ba64-6228ca2df79d",
   "metadata": {},
   "source": [
    "## Integrating with SQL: Using Spark SQL queries with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5faa5-70d8-44a7-8766-45647dcc7cef",
   "metadata": {},
   "source": [
    "Spark SQL provides a powerful way to integrate SQL queries directly into PySpark applications, allowing you to leverage familiar SQL syntax while maintaining the performance benefits of Spark's distributed computing. The integration works through temporary views that register DataFrames as SQL tables, enabling seamless switching between DataFrame API and SQL syntax within the same application. This approach is particularly valuable for teams with strong SQL backgrounds or when working with complex analytical queries that are more naturally expressed in SQL.\n",
    "\n",
    "The core integration mechanism involves registering DataFrames as temporary views using `createOrReplaceTempView()`, then executing SQL queries with `spark.sql()` which returns DataFrames that can be further processed using the DataFrame API. This bidirectional integration allows you to use SQL for complex joins, window functions, and analytical queries while leveraging Python's DataFrame API for data manipulation, machine learning pipelines, and custom transformations. Spark SQL supports the full SQL standard including CTEs, subqueries, window functions, and advanced analytical functions, all optimized by the same Catalyst optimizer that powers the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be1377-3aed-4976-af35-e5fa16e98e96",
   "metadata": {},
   "source": [
    "**Register DataFrames as Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f837d03-2a0c-4a2c-a621-9f7d93d08486",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.createOrReplaceTempView(\"customers\")\n",
    "orders.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09cc5-e754-4df2-861b-7059aa1f6368",
   "metadata": {},
   "source": [
    "**Execute SQL Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd3e07-2fbf-485d-a982-c3522003c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT c.CompanyName, COUNT(o.OrderID) as OrderCount\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.CustomerID = o.CustomerID\n",
    "    GROUP BY c.CompanyName\n",
    "    ORDER BY OrderCount DESC\n",
    "\"\"\")\n",
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21eefd3-ddd7-4c0a-97be-120167a0e084",
   "metadata": {},
   "source": [
    "**Mix SQL and DataFrame API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec4923-d949-4d38-88a2-a4148778d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with SQL\n",
    "sql_result = spark.sql(\"SELECT * FROM customers WHERE Country = 'USA'\")\n",
    "# Continue with DataFrame API\n",
    "final_result = sql_result.filter(col(\"Region\").isNotNull()).orderBy(\"CompanyName\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ddd40-ae76-4a1a-b078-dc01c0071124",
   "metadata": {},
   "source": [
    "**Complex Analytics with CTEs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c91d64-bd93-45da-acda-1a1076fbb1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        SELECT YEAR(OrderDate) as year, MONTH(OrderDate) as month, SUM(Freight) as total\n",
    "        FROM orders GROUP BY YEAR(OrderDate), MONTH(OrderDate)\n",
    "    )\n",
    "    SELECT *, LAG(total) OVER (ORDER BY year, month) as prev_month\n",
    "    FROM monthly_sales\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf94981-892c-4d95-9a46-58f1ea2f0528",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ea51f-d40f-45f5-930d-8f649cad8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
