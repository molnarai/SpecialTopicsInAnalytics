{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b034ca3-f604-4879-b2bf-9097e9095b05",
   "metadata": {},
   "source": [
    "# PySpark Essentials\n",
    "\n",
    "- Spark vs Pandas: Why and when to use Spark\n",
    "- Introduction to Spark DataFrames: Schema, loading data, inspecting DataFrames\n",
    "- Distributed computing basics in Spark: Partitions and transformations\n",
    "- Advanced DataFrame operations:\n",
    "    - Column and row manipulations\n",
    "    - Complex filtering and conditional logic\n",
    "    - GroupBy, aggregations\n",
    "    - Joins and window functions\n",
    "- Handling missing data and data types\n",
    "- Integrating with SQL: Using Spark SQL queries with Python\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb71acb-f739-4df4-b948-18b5b5724c02",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f2b164-2814-4c9e-b5e5-c0685754708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    DateType,\n",
    ")\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147c440e-7fd2-46d8-ac98-123f799002e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/17 19:09:15 WARN Utils: Your hostname, RCBM8368-DIII.local, resolves to a loopback address: 127.0.0.1; using 10.250.4.136 instead (on interface en0)\n",
      "25/09/17 19:09:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/17 19:09:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54280)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Run locally, use four worker nodes. Use this for code development and small problems\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "\n",
    "### Connect to a Spark cluster (e.g. start-connect-server.sh)\n",
    "# SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n",
    "# spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868656e1-d3f9-41af-8d7e-096de6d10cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7fb4ee-8d62-46a7-ab28-274dd02a5677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.250.4.136:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16e4a7750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b165cd68-dcdd-4b93-84c3-f6032b7ff4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0927b88-7686-4621-94ec-44b93b3d4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15d969-3e57-4065-be87-cde28ce7e2d1",
   "metadata": {},
   "source": [
    "## Spark vs Pandas: Why and when to use Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e762b-ebf7-479e-ab6c-a216c8dcba42",
   "metadata": {},
   "source": [
    "## Distributed computing basics in Spark: Partitions and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373f2d2-d041-457a-93b4-a5a72af7da05",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "![](northwind-schema.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fe210c-5666-4f95-8c6e-c9b27c06de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 648\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users     425 Sep 17 12:33 categories.csv\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users   11558 Sep 17 12:33 customers.csv\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users    4085 Sep 17 12:33 employees.csv\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users  295065 Sep 17 12:33 orders.csv\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users    4327 Sep 17 12:33 products.csv\n",
      "-rw-r--r--@ 1 pmolnar  GSUAD\\Domain Users    4007 Sep 17 12:33 suppliers.csv\n"
     ]
    }
   ],
   "source": [
    "! ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766f40cd-ea8c-427e-b19f-e9d801d035ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Method 1: Auto-inference ===\n",
      "root\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Picture: string (nullable = true)\n",
      "\n",
      "+----------+--------------+--------------------+-------+\n",
      "|CategoryID|  CategoryName|         Description|Picture|\n",
      "+----------+--------------+--------------------+-------+\n",
      "|         1|     Beverages|Soft drinks, coff...|     \\x|\n",
      "|         2|    Condiments|Sweet and savory ...|     \\x|\n",
      "|         3|   Confections|Desserts, candies...|     \\x|\n",
      "|         4|Dairy Products|             Cheeses|     \\x|\n",
      "|         5|Grains/Cereals|Breads, crackers,...|     \\x|\n",
      "+----------+--------------+--------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/\"\n",
    "\n",
    "# Method 1: Auto-inference (simple but slower)\n",
    "print(\"=== Method 1: Auto-inference ===\")\n",
    "categories_auto = spark.read.csv(f\"{data_path}categories.csv\", header=True, inferSchema=True)\n",
    "categories_auto.printSchema()\n",
    "categories_auto.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad545fb7-6ccc-4c57-858c-e78fb30201d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 2: Predefined Schema ===\n",
      "root\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Picture: string (nullable = true)\n",
      "\n",
      "+----------+--------------+--------------------+-------+\n",
      "|CategoryID|  CategoryName|         Description|Picture|\n",
      "+----------+--------------+--------------------+-------+\n",
      "|         1|     Beverages|Soft drinks, coff...|     \\x|\n",
      "|         2|    Condiments|Sweet and savory ...|     \\x|\n",
      "|         3|   Confections|Desserts, candies...|     \\x|\n",
      "|         4|Dairy Products|             Cheeses|     \\x|\n",
      "|         5|Grains/Cereals|Breads, crackers,...|     \\x|\n",
      "+----------+--------------+--------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Define schema (faster and more control)\n",
    "print(\"\\n=== Method 2: Predefined Schema ===\")\n",
    "\n",
    "# Categories schema\n",
    "categories_schema = StructType([\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"CategoryName\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Picture\", StringType(), True)\n",
    "])\n",
    "\n",
    "categories_schema_df = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "categories_schema_df.printSchema()\n",
    "categories_schema_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedc6f61-9463-4fc8-938b-6c0e59eea97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 3: Load as Pandas Dataframe  ===\n",
      "root\n",
      " |-- CategoryID: long (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Picture: string (nullable = true)\n",
      "\n",
      "+----------+--------------+--------------------+-------+\n",
      "|CategoryID|  CategoryName|         Description|Picture|\n",
      "+----------+--------------+--------------------+-------+\n",
      "|         1|     Beverages|Soft drinks, coff...|     \\x|\n",
      "|         2|    Condiments|Sweet and savory ...|     \\x|\n",
      "|         3|   Confections|Desserts, candies...|     \\x|\n",
      "|         4|Dairy Products|             Cheeses|     \\x|\n",
      "|         5|Grains/Cereals|Breads, crackers,...|     \\x|\n",
      "+----------+--------------+--------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Load as Pandas Dataframe \n",
    "print(\"\\n=== Method 3: Load as Pandas Dataframe  ===\")\n",
    "\n",
    "cat_df = pd.read_csv(f\"{data_path}categories.csv\")\n",
    "categories_pandas_df = spark.createDataFrame(cat_df)\n",
    "categories_pandas_df.printSchema()\n",
    "categories_pandas_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018805a-7630-40b2-8577-e2fe88d7f858",
   "metadata": {},
   "source": [
    "### Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8191ebeb-212e-47a8-8aca-ba3a89b805f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories schema\n",
    "categories_schema = StructType([\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"CategoryName\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Picture\", StringType(), True)\n",
    "])\n",
    "\n",
    "# categories_schema_df = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "# categories_schema_df.printSchema()\n",
    "# categories_schema_df.show(5)\n",
    "\n",
    "# Customers schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CompanyName\", StringType(), True),\n",
    "    StructField(\"ContactName\", StringType(), True),\n",
    "    StructField(\"ContactTitle\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"PostalCode\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Phone\", StringType(), True),\n",
    "    StructField(\"Fax\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Orders schema\n",
    "orders_schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"RequiredDate\", DateType(), True),\n",
    "    StructField(\"ShippedDate\", DateType(), True),\n",
    "    StructField(\"ShipVia\", IntegerType(), True),\n",
    "    StructField(\"Freight\", FloatType(), True),\n",
    "    StructField(\"ShipName\", StringType(), True),\n",
    "    StructField(\"ShipAddress\", StringType(), True),\n",
    "    StructField(\"ShipCity\", StringType(), True),\n",
    "    StructField(\"ShipRegion\", StringType(), True),\n",
    "    StructField(\"ShipPostalCode\", StringType(), True),\n",
    "    StructField(\"ShipCountry\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Products schema\n",
    "products_schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"SupplierID\", IntegerType(), True),\n",
    "    StructField(\"CategoryID\", IntegerType(), True),\n",
    "    StructField(\"QuantityPerUnit\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"UnitsInStock\", IntegerType(), True),\n",
    "    StructField(\"UnitsOnOrder\", IntegerType(), True),\n",
    "    StructField(\"ReorderLevel\", IntegerType(), True),\n",
    "    StructField(\"Discontinued\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Employees schema\n",
    "employees_schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"LastName\", StringType(), True),\n",
    "    StructField(\"FirstName\", StringType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"TitleOfCourtesy\", StringType(), True),\n",
    "    StructField(\"BirthDate\", DateType(), True),\n",
    "    StructField(\"HireDate\", DateType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"PostalCode\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"HomePhone\", StringType(), True),\n",
    "    StructField(\"Extension\", StringType(), True),\n",
    "    StructField(\"Photo\", StringType(), True),\n",
    "    StructField(\"Notes\", StringType(), True),\n",
    "    StructField(\"ReportsTo\", IntegerType(), True),\n",
    "    StructField(\"PhotoPath\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d1291-a45c-45d2-9306-5f7f9a3fbe35",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f555df5-978c-43ea-bf16-0adf345f04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading all tables with schemas ===\n",
      "Categories:\n",
      "root\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Picture: string (nullable = true)\n",
      "\n",
      "Count: 8\n",
      "Customers:\n",
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- ContactName: string (nullable = true)\n",
      " |-- ContactTitle: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Fax: string (nullable = true)\n",
      "\n",
      "Count: 91\n",
      "\n",
      "Orders:\n",
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- OrderDate: date (nullable = true)\n",
      " |-- RequiredDate: date (nullable = true)\n",
      " |-- ShippedDate: date (nullable = true)\n",
      " |-- ShipVia: integer (nullable = true)\n",
      " |-- Freight: float (nullable = true)\n",
      " |-- ShipName: string (nullable = true)\n",
      " |-- ShipAddress: string (nullable = true)\n",
      " |-- ShipCity: string (nullable = true)\n",
      " |-- ShipRegion: string (nullable = true)\n",
      " |-- ShipPostalCode: string (nullable = true)\n",
      " |-- ShipCountry: string (nullable = true)\n",
      "\n",
      "Count: 2155\n",
      "\n",
      "Products:\n",
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- SupplierID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- QuantityPerUnit: string (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- UnitsInStock: integer (nullable = true)\n",
      " |-- UnitsOnOrder: integer (nullable = true)\n",
      " |-- ReorderLevel: integer (nullable = true)\n",
      " |-- Discontinued: integer (nullable = true)\n",
      "\n",
      "Count: 77\n",
      "\\Employees:\n",
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- SupplierID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- QuantityPerUnit: string (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- UnitsInStock: integer (nullable = true)\n",
      " |-- UnitsOnOrder: integer (nullable = true)\n",
      " |-- ReorderLevel: integer (nullable = true)\n",
      " |-- Discontinued: integer (nullable = true)\n",
      "\n",
      "Count: 9\n"
     ]
    }
   ],
   "source": [
    "# Load all tables with schemas\n",
    "print(\"\\n=== Loading all tables with schemas ===\")\n",
    "categories = spark.read.csv(f\"{data_path}categories.csv\", header=True, schema=categories_schema)\n",
    "customers = spark.read.csv(f\"{data_path}customers.csv\", header=True, schema=customers_schema)\n",
    "orders = spark.read.csv(f\"{data_path}orders.csv\", header=True, schema=orders_schema)\n",
    "products = spark.read.csv(f\"{data_path}products.csv\", header=True, schema=products_schema)\n",
    "employees = spark.read.csv(f\"{data_path}employees.csv\", header=True, schema=employees_schema)\n",
    "\n",
    "\n",
    "print(\"Categories:\")\n",
    "categories.printSchema()\n",
    "print(f\"Count: {categories.count()}\")\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers.printSchema()\n",
    "print(f\"Count: {customers.count()}\")\n",
    "\n",
    "print(\"\\nOrders:\")\n",
    "orders.printSchema()\n",
    "print(f\"Count: {orders.count()}\")\n",
    "\n",
    "print(\"\\nProducts:\")\n",
    "products.printSchema()\n",
    "print(f\"Count: {products.count()}\")\n",
    "\n",
    "print(\"\\Employees:\")\n",
    "products.printSchema()\n",
    "print(f\"Count: {employees.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe17a5-dd85-45d5-a449-d6d96bac0a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372de6e8-c1f7-4771-92ee-946e7d905eeb",
   "metadata": {},
   "source": [
    "## Advanced DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf9540-60a3-48a1-9d34-15dbea88d220",
   "metadata": {},
   "source": [
    "### Column and row manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3386758-e318-4a5d-a600-6f945de2875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, upper, concat, lit, year, month, round, desc\n",
    "# Alternatively...\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e3abd-f5b4-4b43-b417-29771eb100d4",
   "metadata": {},
   "source": [
    "Select Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af1c7a3-0d41-4fac-9f1a-03ac2011fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|CustomerID|         CompanyName|Country|\n",
      "+----------+--------------------+-------+\n",
      "|     ALFKI| Alfreds Futterkiste|Germany|\n",
      "|     ANATR|Ana Trujillo Empa...| Mexico|\n",
      "|     ANTON|Antonio Moreno Ta...| Mexico|\n",
      "|     AROUT|     Around the Horn|     UK|\n",
      "|     BERGS|  Berglunds snabbköp| Sweden|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Extract only customer ID, company name, and country\n",
    "customers.select(\"CustomerID\", \"CompanyName\", \"Country\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddc074-f38c-49bd-a664-c689ef34b953",
   "metadata": {},
   "source": [
    "Add New Column with Literal Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034da1b5-34a1-4230-a262-bad72e166ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+\n",
      "|CustomerID|         CompanyName|Status|\n",
      "+----------+--------------------+------+\n",
      "|     ALFKI| Alfreds Futterkiste|Active|\n",
      "|     ANATR|Ana Trujillo Empa...|Active|\n",
      "|     ANTON|Antonio Moreno Ta...|Active|\n",
      "|     AROUT|     Around the Horn|Active|\n",
      "|     BERGS|  Berglunds snabbköp|Active|\n",
      "+----------+--------------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add a constant 'Active' status column to all customers\n",
    "customers_with_status = customers.withColumn(\"Status\", F.lit(\"Active\"))\n",
    "customers_with_status.select(\"CustomerID\", \"CompanyName\", \"Status\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4e6fa-5e78-42f8-af47-cc21a5adec67",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21b7b129-4698-43e3-bbb6-58885dd36aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+\n",
      "|   ID|             Company|Country|\n",
      "+-----+--------------------+-------+\n",
      "|ALFKI| Alfreds Futterkiste|Germany|\n",
      "|ANATR|Ana Trujillo Empa...| Mexico|\n",
      "|ANTON|Antonio Moreno Ta...| Mexico|\n",
      "|AROUT|     Around the Horn|     UK|\n",
      "|BERGS|  Berglunds snabbköp| Sweden|\n",
      "+-----+--------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Rename CustomerID to ID and CompanyName to Company\n",
    "customers_renamed = customers.withColumnRenamed(\"CustomerID\", \"ID\").withColumnRenamed(\"CompanyName\", \"Company\")\n",
    "customers_renamed.select(\"ID\", \"Company\", \"Country\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0f723-5bd0-4501-a6d4-c62b1a3a1813",
   "metadata": {},
   "source": [
    "Conditional Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c6f6ed9-4b42-4257-be83-0ad2d1baa232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+\n",
      "|         ProductName|UnitPrice|PriceCategory|\n",
      "+--------------------+---------+-------------+\n",
      "|                Chai|     18.0|   Affordable|\n",
      "|               Chang|     19.0|   Affordable|\n",
      "|       Aniseed Syrup|     10.0|   Affordable|\n",
      "|Chef Anton's Caju...|     22.0|    Expensive|\n",
      "|Chef Anton's Gumb...|    21.35|    Expensive|\n",
      "+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Categorize products as 'Expensive' if price > 20, else 'Affordable'\n",
    "products_categorized = products.withColumn(\"PriceCategory\", \n",
    "    F.when(F.col(\"UnitPrice\") > 20, \"Expensive\").otherwise(\"Affordable\"))\n",
    "products_categorized.select(\"ProductName\", \"UnitPrice\", \"PriceCategory\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013eb338-7fef-486e-ac70-ab4f73f8a27a",
   "metadata": {},
   "source": [
    "String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72d9c57d-b9f3-43b3-9174-6db4bbf70e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------------------------------------+\n",
      "|CompanyUpper                      |FullAddress                                       |\n",
      "+----------------------------------+--------------------------------------------------+\n",
      "|ALFREDS FUTTERKISTE               |Obere Str. 57, Berlin, Germany                    |\n",
      "|ANA TRUJILLO EMPAREDADOS Y HELADOS|Avda. de la Constitución 2222, México D.F., Mexico|\n",
      "|ANTONIO MORENO TAQUERÍA           |Mataderos  2312, México D.F., Mexico              |\n",
      "|AROUND THE HORN                   |120 Hanover Sq., London, UK                       |\n",
      "|BERGLUNDS SNABBKÖP                |Berguvsvägen  8, Luleå, Sweden                    |\n",
      "+----------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Convert company names to uppercase and create full address\n",
    "customers_formatted = customers.withColumn(\"CompanyUpper\", F.upper(F.col(\"CompanyName\"))) \\\n",
    "    .withColumn(\"FullAddress\", F.concat(F.col(\"Address\"), F.lit(\", \"), F.col(\"City\"), F.lit(\", \"), F.col(\"Country\")))\n",
    "customers_formatted.select(\"CompanyUpper\", \"FullAddress\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411eb3d8-992e-4b34-afb2-bb153d3694be",
   "metadata": {},
   "source": [
    "Filter Rows Based on Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9e926-630a-4af8-a9b3-23e5547aef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only customers from USA or Germany\n",
    "filtered_customers = customers.filter((col(\"Country\") == \"USA\") | (col(\"Country\") == \"Germany\"))\n",
    "filtered_customers.select(\"CompanyName\", \"Country\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f762640-68e8-4535-a90c-bdc1b1614b69",
   "metadata": {},
   "source": [
    "Extract Date Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798c89b-1741-454a-b46a-dc5f86efbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from order dates\n",
    "orders_with_date_parts = orders.withColumn(\"OrderYear\", year(col(\"OrderDate\"))) \\\n",
    "    .withColumn(\"OrderMonth\", month(col(\"OrderDate\")))\n",
    "orders_with_date_parts.select(\"OrderID\", \"OrderDate\", \"OrderYear\", \"OrderMonth\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a8e1b-11ca-4387-8ba6-c17fb0161d3c",
   "metadata": {},
   "source": [
    "Round Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c82e85-0924-4137-94d2-2539eb73b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round freight costs to 2 decimal places and create freight categories\n",
    "orders_rounded = orders.withColumn(\"FreightRounded\", round(col(\"Freight\"), 2)) \\\n",
    "    .withColumn(\"FreightCategory\", \n",
    "        when(col(\"Freight\") < 10, \"Low\")\n",
    "        .when(col(\"Freight\") < 50, \"Medium\")\n",
    "        .otherwise(\"High\"))\n",
    "orders_rounded.select(\"OrderID\", \"Freight\", \"FreightRounded\", \"FreightCategory\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd8880-f1cc-4ed8-a68e-57ffd1dbea14",
   "metadata": {},
   "source": [
    "Sort and Limit Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af1209-67ab-48f4-b3e9-4bae5804efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 most expensive products, sorted by price descending\n",
    "expensive_products = products.orderBy(desc(\"UnitPrice\")).limit(5)\n",
    "expensive_products.select(\"ProductName\", \"UnitPrice\", \"CategoryID\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d64e3-d948-49da-9c05-5d31f915abb4",
   "metadata": {},
   "source": [
    "### Complex filtering and conditional logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45549bf7-7fb3-4254-9416-e37e02ec470c",
   "metadata": {},
   "source": [
    "**Multiple AND/OR Conditions with Null Handling**\n",
    "\n",
    "This combines multiple conditions using AND (&) and OR (|) operators. It finds customers who either: (1) are from specific European countries AND have both phone and fax numbers, OR (2) are from the USA. The isNotNull() function handles missing data gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481489e-3be0-4e34-901e-dd2edfa5f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segments = customers.withColumn(\"Segment\",\n",
    "    when(col(\"Country\") == \"USA\", \n",
    "         when(col(\"Region\").isNotNull(), \"US-Regional\")\n",
    "         .otherwise(\"US-National\"))\n",
    "    .when(col(\"Country\").isin(\"Germany\", \"France\", \"UK\"), \"EU-Premium\")\n",
    "    .when(col(\"Country\").isin(\"Brazil\", \"Argentina\", \"Mexico\"), \"LATAM\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "customer_segments.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360b3b1-2af6-424d-ac26-29b5046c872b",
   "metadata": {},
   "source": [
    "**Nested Conditional Logic (CASE-WHEN)**\n",
    "\n",
    "This creates nested conditional logic similar to SQL's CASE-WHEN. It segments customers into tiers based on country and region, with nested conditions for US customers (regional vs national) and different categories for other geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7fd6a67-a661-46bd-bf73-ecb921c3bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+----------+\n",
      "|CustomerID|         CompanyName|   ContactName|        ContactTitle|             Address|       City|Region|PostalCode|Country|       Phone|         Fax|   Segment|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+----------+\n",
      "|     ALFKI| Alfreds Futterkiste|  Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  NULL|     12209|Germany| 030-0074321| 030-0076545|EU-Premium|\n",
      "|     ANATR|Ana Trujillo Empa...|  Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  NULL|     05021| Mexico|(5) 555-4729|(5) 555-3745|     LATAM|\n",
      "|     ANTON|Antonio Moreno Ta...|Antonio Moreno|               Owner|     Mataderos  2312|México D.F.|  NULL|     05023| Mexico|(5) 555-3932|        NULL|     LATAM|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "customer_segments = customers.withColumn(\"Segment\",\n",
    "    when(col(\"Country\") == \"USA\", \n",
    "         when(col(\"Region\").isNotNull(), \"US-Regional\")\n",
    "         .otherwise(\"US-National\"))\n",
    "    .when(col(\"Country\").isin(\"Germany\", \"France\", \"UK\"), \"EU-Premium\")\n",
    "    .when(col(\"Country\").isin(\"Brazil\", \"Argentina\", \"Mexico\"), \"LATAM\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "customer_segments.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465dcee-ebef-4482-bfa7-549f57235fb4",
   "metadata": {},
   "source": [
    "**Date-Based Filtering with Calculations**\n",
    "\n",
    "This identifies problematic orders using date calculations. It finds orders that are: (1) shipped more than 7 days late, (2) have high freight costs for international shipments, or (3) remain unshipped after 30 days. The datediff() function calculates differences between dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b69ae-10a7-4973-a438-2f99540ed3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "problematic_orders = orders.filter(\n",
    "    (datediff(col(\"ShippedDate\"), col(\"RequiredDate\")) > 7) |\n",
    "    ((col(\"Freight\") > 100) & (col(\"ShipCountry\") != \"USA\")) |\n",
    "    (col(\"ShippedDate\").isNull() & (datediff(current_date(), col(\"OrderDate\")) > 30))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee82db-aaa2-4a5b-a02b-0007a1166e46",
   "metadata": {},
   "source": [
    "**String Pattern Matching with Conditional Transformations**\n",
    "\n",
    "This uses regular expressions to categorize products based on name patterns. The `(?i)` flag makes the search case-insensitive, and the pipe `(|)` acts as OR within the regex. It then applies different pricing strategies based on the categorization and other business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22057c-a10b-469d-9595-69743d0a6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "product_categories = products.withColumn(\"ProductCategory\",\n",
    "    when(col(\"ProductName\").rlike(\"(?i).*(cheese|dairy).*\"), \"Dairy\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(wine|beer|ale).*\"), \"Alcoholic\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(sauce|syrup|spread).*\"), \"Condiments\")\n",
    "    .when(col(\"ProductName\").rlike(\"(?i).*(tea|coffee).*\"), \"Beverages\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898c028-6bc5-4feb-863e-81c91f3bb91d",
   "metadata": {},
   "source": [
    "**Complex Inventory Management Logic**\n",
    "\n",
    "This implements complex business logic for inventory management. It creates alerts for different scenarios: out-of-stock active products, low stock below reorder levels, expensive overstocked items, and products that need reordering. The final filter removes \"Normal\" items to focus only on products requiring attention.\n",
    "\n",
    "The .rlike() method performs regex matching on column values. The pattern (?i).*(cheese|dairy).* means:\n",
    "\n",
    "- `(?i)` - Case insensitive flag\n",
    "- `.*` - Match any characters before\n",
    "- `(cheese|dairy)` - Match either \"cheese\" or \"dairy\"\n",
    "- `.*` - Match any characters after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6be88065-324a-4348-b8f8-9d36b8db2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- SupplierID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- QuantityPerUnit: string (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- UnitsInStock: integer (nullable = true)\n",
      " |-- UnitsOnOrder: integer (nullable = true)\n",
      " |-- ReorderLevel: integer (nullable = true)\n",
      " |-- Discontinued: integer (nullable = true)\n",
      " |-- AlertType: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_alerts = products.withColumn(\"AlertType\",\n",
    "    when((col(\"UnitsInStock\") == 0) & (col(\"Discontinued\") == 0), \"Out of Stock\")\n",
    "    .when((col(\"UnitsInStock\") < col(\"ReorderLevel\")) & (col(\"Discontinued\") == 0), \"Low Stock\")\n",
    "    .when((col(\"UnitsInStock\") > 100) & (col(\"UnitPrice\") > 20), \"Overstock Expensive\")\n",
    "    .when((col(\"UnitsOnOrder\") == 0) & (col(\"UnitsInStock\") < 20) & (col(\"Discontinued\") == 0), \"No Reorder\")\n",
    "    .otherwise(\"Normal\")\n",
    ").filter(col(\"AlertType\") != \"Normal\")\n",
    "inventory_alerts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b95f345-6b66-479b-a952-b878050b10cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+----------+-------------------+---------+------------+------------+------------+------------+-------------------+\n",
      "|ProductID|         ProductName|SupplierID|CategoryID|    QuantityPerUnit|UnitPrice|UnitsInStock|UnitsOnOrder|ReorderLevel|Discontinued|          AlertType|\n",
      "+---------+--------------------+----------+----------+-------------------+---------+------------+------------+------------+------------+-------------------+\n",
      "|        3|       Aniseed Syrup|         1|         2|12 - 550 ml bottles|     10.0|          13|          70|          25|           0|          Low Stock|\n",
      "|        6|Grandma's Boysenb...|         3|         2|     12 - 8 oz jars|     25.0|         120|           0|          25|           0|Overstock Expensive|\n",
      "|        7|Uncle Bob's Organ...|         3|         7|    12 - 1 lb pkgs.|     30.0|          15|           0|          10|           0|         No Reorder|\n",
      "+---------+--------------------+----------+----------+-------------------+---------+------------+------------+------------+------------+-------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "inventory_alerts.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95ae53f3-c7a2-4aac-ae0e-4a564963dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory_alerts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31f70abc-8cd7-4a0c-ac58-b054a24b2243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory_alerts.cache()\n",
    "inventory_alerts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10166d2a-c98c-45fd-a539-2d94149b297b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory_alerts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcfae0-b249-4ee3-8271-95c4fcdedfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbba63-5b7c-4eb8-80a0-d5a8a09d8451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc7bad6-d120-4c4e-af5c-36838a3c559c",
   "metadata": {},
   "source": [
    "### Joins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6e873-c2e4-4c52-9dd9-76bd86b95daf",
   "metadata": {},
   "source": [
    "**Inner Join - Basic Relationship**\n",
    "\n",
    "This performs a standard inner join between orders and customers tables on the CustomerID column. Only orders that have matching customers are returned. This is the most common join type and provides the best performance since it filters out non-matching records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f78b6-5a19-4587-84b7-68d034563c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_customers = orders.join(customers, \"CustomerID\", \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b681-0d57-4754-a62a-d48d6117a721",
   "metadata": {},
   "source": [
    "**Left Join - Preserve All Records**\n",
    "\n",
    "A left join preserves all customers even if they have no orders. The aggregation counts orders per customer, showing 0 for customers without orders. This is useful for finding inactive customers or getting complete customer metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b00bb499-85d8-4c9f-b64c-bac434fb05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "customer_orders = customers.join(orders, \"CustomerID\", \"left\") \\\n",
    "    .groupBy(\"CustomerID\", \"CompanyName\") \\\n",
    "    .agg(count(\"OrderID\").alias(\"OrderCount\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72af5ae9-7708-44b8-8e9d-d3a8a0b25bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- OrderCount: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d85c65-c02c-4278-8349-7e1242d32c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|CustomerID|         CompanyName|OrderCount|\n",
      "+----------+--------------------+----------+\n",
      "|     CENTC|Centro comercial ...|         2|\n",
      "|     COMMI|    Comércio Mineiro|        10|\n",
      "|     OCEAN|Océano Atlántico ...|        11|\n",
      "|     ANATR|Ana Trujillo Empa...|        10|\n",
      "|     LACOR|La corne d'abondance|        11|\n",
      "|     ERNSH|        Ernst Handel|       102|\n",
      "|     FRANS|      Franchi S.p.A.|        10|\n",
      "|     GROSR|GROSELLA-Restaurante|         4|\n",
      "|     QUEDE|         Que Delícia|        24|\n",
      "|     TOMSP|  Toms Spezialitäten|        14|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "customer_orders.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a6730-681c-41ab-9c5e-fe8d571ace92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49368ee3-5d77-41dc-bc00-a9efb3f34d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9c77c-3f0d-4b3b-917e-71dadc88c1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b9fc87c-a9d6-4b92-9675-6ce4f4e4c9cb",
   "metadata": {},
   "source": [
    "**Broadcast Join - Small Table Optimization**\n",
    "\n",
    "The `broadcast()` function tells Spark to send the small categories table to all worker nodes, avoiding expensive shuffle operations. This is highly efficient when one table is small (<200MB). Spark automatically broadcasts tables under 10MB, but explicit broadcasting ensures optimization for larger small tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d5c0c-420c-457f-8009-50bb69659f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "products_categories = products.join(broadcast(categories), \"CategoryID\", \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276bc5-73a4-4aa7-8f33-cced19d00430",
   "metadata": {},
   "source": [
    "**Multiple Table Join - Complex Business Query**\n",
    "\n",
    "This chains multiple joins to create a comprehensive view. Note the explicit column reference `(orders.EmployeeID == employees.EmployeeID)` when column names might be ambiguous. This pattern is common in data warehousing for creating denormalized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1863ffa-221d-407c-8eb5-f62b3d780bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_orders = orders \\\n",
    "    .join(customers, \"CustomerID\", \"inner\") \\\n",
    "    .join(employees, orders.EmployeeID == employees.EmployeeID, \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2c97d-3a11-44e9-bacd-d87140e3bf2f",
   "metadata": {},
   "source": [
    "**Inequality Join - Range-Based Matching**\n",
    "\n",
    "Unlike equality joins, this uses range conditions to match products to price tiers. The join condition `(UnitPrice >= MinPrice) & (UnitPrice < MaxPrice)` assigns each product to its appropri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eccea9-ca4f-47fd-b9c6-cab0108a7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price tiers DataFrame\n",
    "price_tiers = spark.createDataFrame([\n",
    "    (1, \"Budget\", 0.0, 10.0),\n",
    "    (2, \"Standard\", 10.0, 25.0),\n",
    "    (3, \"Premium\", 25.0, 50.0),\n",
    "    (4, \"Luxury\", 50.0, 999.0)\n",
    "], [\"TierID\", \"TierName\", \"MinPrice\", \"MaxPrice\"])\n",
    "\n",
    "products_tiers = products.join(\n",
    "    price_tiers,\n",
    "    (col(\"UnitPrice\") >= col(\"MinPrice\")) & (col(\"UnitPrice\") < col(\"MaxPrice\")),\n",
    "    \"inner\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e636795-062b-4df1-8a29-35f3f44183e6",
   "metadata": {},
   "source": [
    "#### Attributes vs Col\n",
    "\n",
    "This FAILS - no schema defined\n",
    "    df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")])\n",
    "    result = df.CustomerID  # AttributeError: 'DataFrame' object has no attribute 'CustomerID'\n",
    "\n",
    "\n",
    "This WORKS - schema defined\n",
    "    schema = StructType([\n",
    "        StructField(\"CustomerID\", IntegerType(), True),\n",
    "        StructField(\"Name\", StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], schema)\n",
    "    result = df.CustomerID  # Works!\n",
    "\n",
    "Why This Happens\n",
    "- Python's Dynamic Nature: Python resolves attributes at runtime using __getattr__\n",
    "- Unknown Column Names: Without schema, Spark doesn't know what columns exist\n",
    "- Lazy Evaluation: Schema inference happens only when an action is triggered\n",
    "\n",
    "Solutions\n",
    "- Use `col()` function (always works):\n",
    "        ```df.select(col(\"CustomerID\"))  # Works with or without schema```\n",
    "- Use bracket notation:\n",
    "        ```df[\"CustomerID\"]  # Works with or without schema```\n",
    "- Define schema explicitly:\n",
    "        ```df = spark.createDataFrame(data, schema)\n",
    "        df.CustomerID  # Now works```\n",
    "\n",
    "The attribute notation (df.ColumnName) is syntactic sugar that only works when Spark knows the column exists at DataFrame creation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0757c1-a1d8-47ce-85c4-3e0c0f138c21",
   "metadata": {},
   "source": [
    "### GroupBy, aggregations and window functions\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- Grouping: Collapses rows into groups for aggregation\n",
    "- Window Functions: Perform calculations across related rows without collapsing\n",
    "- Partitioning: Divides data into logical groups for separate processing\n",
    "- Ordering: Determines sequence for ranking and frame-based operations\n",
    "- Frames: Define which rows to include in window calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d2176-c1b1-48c8-8aa9-dfc6ce060564",
   "metadata": {},
   "source": [
    " **Basic Grouping and Aggregation**\n",
    "\n",
    "Groups orders by shipping country and calculates basic statistics. The agg() function allows multiple aggregations in one operation, providing count, average, and sum of freight costs per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec03493-2606-4c2c-8c18-c052b2f60a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ShipCountry: string (nullable = true)\n",
      " |-- OrderCount: long (nullable = false)\n",
      " |-- AvgFreight: double (nullable = true)\n",
      " |-- TotalFreight: double (nullable = true)\n",
      "\n",
      "Number of records: 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShipCountry</th>\n",
       "      <th>OrderCount</th>\n",
       "      <th>AvgFreight</th>\n",
       "      <th>TotalFreight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>97</td>\n",
       "      <td>104.599175</td>\n",
       "      <td>10146.120006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>328</td>\n",
       "      <td>116.374238</td>\n",
       "      <td>38170.750006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>184</td>\n",
       "      <td>68.622718</td>\n",
       "      <td>12626.580033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>34</td>\n",
       "      <td>52.137353</td>\n",
       "      <td>1772.670002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>56</td>\n",
       "      <td>76.268392</td>\n",
       "      <td>4271.029926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finland</td>\n",
       "      <td>54</td>\n",
       "      <td>53.223889</td>\n",
       "      <td>2874.090020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italy</td>\n",
       "      <td>53</td>\n",
       "      <td>39.790377</td>\n",
       "      <td>2108.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Norway</td>\n",
       "      <td>16</td>\n",
       "      <td>56.065625</td>\n",
       "      <td>897.049995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Spain</td>\n",
       "      <td>54</td>\n",
       "      <td>44.800926</td>\n",
       "      <td>2419.250022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>46</td>\n",
       "      <td>93.184347</td>\n",
       "      <td>4286.479981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>55</td>\n",
       "      <td>131.172544</td>\n",
       "      <td>7214.489899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>118</td>\n",
       "      <td>66.715593</td>\n",
       "      <td>7872.440016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>USA</td>\n",
       "      <td>352</td>\n",
       "      <td>131.152898</td>\n",
       "      <td>46165.820243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>72</td>\n",
       "      <td>44.854305</td>\n",
       "      <td>3229.509961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UK</td>\n",
       "      <td>135</td>\n",
       "      <td>62.892815</td>\n",
       "      <td>8490.529964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>52</td>\n",
       "      <td>75.250384</td>\n",
       "      <td>3913.019987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Canada</td>\n",
       "      <td>75</td>\n",
       "      <td>84.291999</td>\n",
       "      <td>6321.899959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>203</td>\n",
       "      <td>71.837341</td>\n",
       "      <td>14582.980182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Poland</td>\n",
       "      <td>16</td>\n",
       "      <td>28.845625</td>\n",
       "      <td>461.530006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>30</td>\n",
       "      <td>58.317667</td>\n",
       "      <td>1749.530002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Austria</td>\n",
       "      <td>125</td>\n",
       "      <td>221.851521</td>\n",
       "      <td>27731.440125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ShipCountry  OrderCount  AvgFreight  TotalFreight\n",
       "0        Sweden          97  104.599175  10146.120006\n",
       "1       Germany         328  116.374238  38170.750006\n",
       "2        France         184   68.622718  12626.580033\n",
       "3     Argentina          34   52.137353   1772.670002\n",
       "4       Belgium          56   76.268392   4271.029926\n",
       "5       Finland          54   53.223889   2874.090020\n",
       "6         Italy          53   39.790377   2108.890000\n",
       "7        Norway          16   56.065625    897.049995\n",
       "8         Spain          54   44.800926   2419.250022\n",
       "9       Denmark          46   93.184347   4286.479981\n",
       "10      Ireland          55  131.172544   7214.489899\n",
       "11    Venezuela         118   66.715593   7872.440016\n",
       "12          USA         352  131.152898  46165.820243\n",
       "13       Mexico          72   44.854305   3229.509961\n",
       "14           UK         135   62.892815   8490.529964\n",
       "15  Switzerland          52   75.250384   3913.019987\n",
       "16       Canada          75   84.291999   6321.899959\n",
       "17       Brazil         203   71.837341  14582.980182\n",
       "18       Poland          16   28.845625    461.530006\n",
       "19     Portugal          30   58.317667   1749.530002\n",
       "20      Austria         125  221.851521  27731.440125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum # naming conflict when using other package\n",
    "# from pyspark.sql.functions import sum as spark_sum ...or use F.sum()\n",
    "\n",
    "country_stats = orders.groupBy(\"ShipCountry\") \\\n",
    "    .agg(\n",
    "        count(\"OrderID\").alias(\"OrderCount\"),\n",
    "        avg(\"Freight\").alias(\"AvgFreight\"),\n",
    "        sum(\"Freight\").alias(\"TotalFreight\")\n",
    "    )\n",
    "country_stats.printSchema()\n",
    "number_or_rec = country_stats.count()\n",
    "print(f\"Number of records: {number_or_rec:,}\")\n",
    "country_stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89321787-3756-4afe-9a9e-d31e351e9ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ShipCountry='Sweden', OrderCount=97, AvgFreight=104.59917531554233, TotalFreight=10146.120005607605),\n",
       " Row(ShipCountry='Germany', OrderCount=328, AvgFreight=116.374237821869, TotalFreight=38170.750005573034),\n",
       " Row(ShipCountry='France', OrderCount=184, AvgFreight=68.6227175715258, TotalFreight=12626.580033160746),\n",
       " Row(ShipCountry='Argentina', OrderCount=34, AvgFreight=52.137353001271975, TotalFreight=1772.6700020432472),\n",
       " Row(ShipCountry='Belgium', OrderCount=56, AvgFreight=76.26839154213667, TotalFreight=4271.0299263596535),\n",
       " Row(ShipCountry='Finland', OrderCount=54, AvgFreight=53.22388925927657, TotalFreight=2874.0900200009346),\n",
       " Row(ShipCountry='Italy', OrderCount=53, AvgFreight=39.79037735315988, TotalFreight=2108.889999717474),\n",
       " Row(ShipCountry='Norway', OrderCount=16, AvgFreight=56.065624713897705, TotalFreight=897.0499954223633),\n",
       " Row(ShipCountry='Spain', OrderCount=54, AvgFreight=44.80092633212054, TotalFreight=2419.2500219345093),\n",
       " Row(ShipCountry='Denmark', OrderCount=46, AvgFreight=93.18434742222662, TotalFreight=4286.479981422424),\n",
       " Row(ShipCountry='Ireland', OrderCount=55, AvgFreight=131.17254361239347, TotalFreight=7214.489898681641),\n",
       " Row(ShipCountry='Venezuela', OrderCount=118, AvgFreight=66.71559335600774, TotalFreight=7872.4400160089135),\n",
       " Row(ShipCountry='USA', OrderCount=352, AvgFreight=131.1528984177011, TotalFreight=46165.82024303079),\n",
       " Row(ShipCountry='Mexico', OrderCount=72, AvgFreight=44.85430501111679, TotalFreight=3229.5099608004093),\n",
       " Row(ShipCountry='UK', OrderCount=135, AvgFreight=62.8928145452782, TotalFreight=8490.529963612556),\n",
       " Row(ShipCountry='Switzerland', OrderCount=52, AvgFreight=75.25038436972179, TotalFreight=3913.0199872255325),\n",
       " Row(ShipCountry='Canada', OrderCount=75, AvgFreight=84.29199945290884, TotalFreight=6321.8999589681625),\n",
       " Row(ShipCountry='Brazil', OrderCount=203, AvgFreight=71.8373407976968, TotalFreight=14582.98018193245),\n",
       " Row(ShipCountry='Poland', OrderCount=16, AvgFreight=28.845625400543213, TotalFreight=461.5300064086914),\n",
       " Row(ShipCountry='Portugal', OrderCount=30, AvgFreight=58.31766672929128, TotalFreight=1749.5300018787384),\n",
       " Row(ShipCountry='Austria', OrderCount=125, AvgFreight=221.85152100372315, TotalFreight=27731.440125465393)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_stats.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739d33c-fc77-4413-950f-2dd9fdde8efd",
   "metadata": {},
   "source": [
    "**Multiple Column Grouping**\n",
    "\n",
    "Groups by multiple columns (category and supplier) to create a cross-tabulation analysis. This shows how products are distributed across category-supplier combinations with their pricing and inventory metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901abad4-7932-4997-8331-46ff0d599663",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_supplier_stats = products.groupBy(\"CategoryID\", \"SupplierID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"ProductCount\"),\n",
    "        avg(\"UnitPrice\").alias(\"AvgPrice\"),\n",
    "        sum(\"UnitsInStock\").alias(\"TotalStock\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774a389-6240-4555-859a-bd89fbffb41b",
   "metadata": {},
   "source": [
    "**Window Function - Ranking**\n",
    "\n",
    "Creates cumulative sums using window frames. `rowsBetween(unboundedPreceding, currentRow)` includes all rows from the start of the partition to the current row, creating a running total of freight costs per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bce6cb87-6be2-4280-9ee9-441b2b464d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "customer_window = Window.partitionBy(\"CustomerID\").orderBy(\"OrderDate\")\n",
    "running_totals = orders.withColumn(\"RunningFreight\", \n",
    "    sum(\"Freight\").over(customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dce324f9-8e4e-4654-a4b8-fd114bec4989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- OrderDate: date (nullable = true)\n",
      " |-- RequiredDate: date (nullable = true)\n",
      " |-- ShippedDate: date (nullable = true)\n",
      " |-- ShipVia: integer (nullable = true)\n",
      " |-- Freight: float (nullable = true)\n",
      " |-- ShipName: string (nullable = true)\n",
      " |-- ShipAddress: string (nullable = true)\n",
      " |-- ShipCity: string (nullable = true)\n",
      " |-- ShipRegion: string (nullable = true)\n",
      " |-- ShipPostalCode: string (nullable = true)\n",
      " |-- ShipCountry: string (nullable = true)\n",
      " |-- RunningFreight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_totals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9aefb4ff-8e03-434c-ba6b-e3d9ae0828f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 20:10:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 19, schema size: 14\n",
      "CSV file: file:///Users/pmolnar/Classes-Workshops/MSA8395_Special_Topics_in_Analytics/SpecialTopicsInAnalytics/apache_spark/notebooks/data/orders.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>RequiredDate</th>\n",
       "      <th>ShippedDate</th>\n",
       "      <th>ShipVia</th>\n",
       "      <th>Freight</th>\n",
       "      <th>ShipName</th>\n",
       "      <th>ShipAddress</th>\n",
       "      <th>ShipCity</th>\n",
       "      <th>ShipRegion</th>\n",
       "      <th>ShipPostalCode</th>\n",
       "      <th>ShipCountry</th>\n",
       "      <th>RunningFreight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10643</td>\n",
       "      <td>ALFKI</td>\n",
       "      <td>6</td>\n",
       "      <td>1997-08-25</td>\n",
       "      <td>1997-09-22</td>\n",
       "      <td>1997-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>Alfreds Futterkiste</td>\n",
       "      <td>Obere Str. 57</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>None</td>\n",
       "      <td>12209</td>\n",
       "      <td>Germany</td>\n",
       "      <td>29.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10643</td>\n",
       "      <td>ALFKI</td>\n",
       "      <td>6</td>\n",
       "      <td>1997-08-25</td>\n",
       "      <td>1997-09-22</td>\n",
       "      <td>1997-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>Alfreds Futterkiste</td>\n",
       "      <td>Obere Str. 57</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>None</td>\n",
       "      <td>12209</td>\n",
       "      <td>Germany</td>\n",
       "      <td>58.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10643</td>\n",
       "      <td>ALFKI</td>\n",
       "      <td>6</td>\n",
       "      <td>1997-08-25</td>\n",
       "      <td>1997-09-22</td>\n",
       "      <td>1997-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>Alfreds Futterkiste</td>\n",
       "      <td>Obere Str. 57</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>None</td>\n",
       "      <td>12209</td>\n",
       "      <td>Germany</td>\n",
       "      <td>88.379997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10692</td>\n",
       "      <td>ALFKI</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-10-03</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>1997-10-13</td>\n",
       "      <td>2</td>\n",
       "      <td>61.020000</td>\n",
       "      <td>Alfred's Futterkiste</td>\n",
       "      <td>Obere Str. 57</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>None</td>\n",
       "      <td>12209</td>\n",
       "      <td>Germany</td>\n",
       "      <td>149.399998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10702</td>\n",
       "      <td>ALFKI</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-10-13</td>\n",
       "      <td>1997-11-24</td>\n",
       "      <td>1997-10-21</td>\n",
       "      <td>1</td>\n",
       "      <td>23.940001</td>\n",
       "      <td>Alfred's Futterkiste</td>\n",
       "      <td>Obere Str. 57</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>None</td>\n",
       "      <td>12209</td>\n",
       "      <td>Germany</td>\n",
       "      <td>173.339998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>10998</td>\n",
       "      <td>WOLZA</td>\n",
       "      <td>8</td>\n",
       "      <td>1998-04-03</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>2</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>Wolski Zajazd</td>\n",
       "      <td>ul. Filtrowa 68</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>None</td>\n",
       "      <td>01-012</td>\n",
       "      <td>Poland</td>\n",
       "      <td>391.880008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>10998</td>\n",
       "      <td>WOLZA</td>\n",
       "      <td>8</td>\n",
       "      <td>1998-04-03</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>2</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>Wolski Zajazd</td>\n",
       "      <td>ul. Filtrowa 68</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>None</td>\n",
       "      <td>01-012</td>\n",
       "      <td>Poland</td>\n",
       "      <td>412.190007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>10998</td>\n",
       "      <td>WOLZA</td>\n",
       "      <td>8</td>\n",
       "      <td>1998-04-03</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>2</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>Wolski Zajazd</td>\n",
       "      <td>ul. Filtrowa 68</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>None</td>\n",
       "      <td>01-012</td>\n",
       "      <td>Poland</td>\n",
       "      <td>432.500007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>10998</td>\n",
       "      <td>WOLZA</td>\n",
       "      <td>8</td>\n",
       "      <td>1998-04-03</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>1998-04-17</td>\n",
       "      <td>2</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>Wolski Zajazd</td>\n",
       "      <td>ul. Filtrowa 68</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>None</td>\n",
       "      <td>01-012</td>\n",
       "      <td>Poland</td>\n",
       "      <td>452.810006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>11044</td>\n",
       "      <td>WOLZA</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-04-23</td>\n",
       "      <td>1998-05-21</td>\n",
       "      <td>1998-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>Wolski Zajazd</td>\n",
       "      <td>ul. Filtrowa 68</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>None</td>\n",
       "      <td>01-012</td>\n",
       "      <td>Poland</td>\n",
       "      <td>461.530006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2155 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OrderID CustomerID  EmployeeID   OrderDate RequiredDate ShippedDate  \\\n",
       "0       10643      ALFKI           6  1997-08-25   1997-09-22  1997-09-02   \n",
       "1       10643      ALFKI           6  1997-08-25   1997-09-22  1997-09-02   \n",
       "2       10643      ALFKI           6  1997-08-25   1997-09-22  1997-09-02   \n",
       "3       10692      ALFKI           4  1997-10-03   1997-10-31  1997-10-13   \n",
       "4       10702      ALFKI           4  1997-10-13   1997-11-24  1997-10-21   \n",
       "...       ...        ...         ...         ...          ...         ...   \n",
       "2150    10998      WOLZA           8  1998-04-03   1998-04-17  1998-04-17   \n",
       "2151    10998      WOLZA           8  1998-04-03   1998-04-17  1998-04-17   \n",
       "2152    10998      WOLZA           8  1998-04-03   1998-04-17  1998-04-17   \n",
       "2153    10998      WOLZA           8  1998-04-03   1998-04-17  1998-04-17   \n",
       "2154    11044      WOLZA           4  1998-04-23   1998-05-21  1998-05-01   \n",
       "\n",
       "      ShipVia    Freight              ShipName      ShipAddress  ShipCity  \\\n",
       "0           1  29.459999   Alfreds Futterkiste    Obere Str. 57    Berlin   \n",
       "1           1  29.459999   Alfreds Futterkiste    Obere Str. 57    Berlin   \n",
       "2           1  29.459999   Alfreds Futterkiste    Obere Str. 57    Berlin   \n",
       "3           2  61.020000  Alfred's Futterkiste    Obere Str. 57    Berlin   \n",
       "4           1  23.940001  Alfred's Futterkiste    Obere Str. 57    Berlin   \n",
       "...       ...        ...                   ...              ...       ...   \n",
       "2150        2  20.309999         Wolski Zajazd  ul. Filtrowa 68  Warszawa   \n",
       "2151        2  20.309999         Wolski Zajazd  ul. Filtrowa 68  Warszawa   \n",
       "2152        2  20.309999         Wolski Zajazd  ul. Filtrowa 68  Warszawa   \n",
       "2153        2  20.309999         Wolski Zajazd  ul. Filtrowa 68  Warszawa   \n",
       "2154        1   8.720000         Wolski Zajazd  ul. Filtrowa 68  Warszawa   \n",
       "\n",
       "     ShipRegion ShipPostalCode ShipCountry  RunningFreight  \n",
       "0          None          12209     Germany       29.459999  \n",
       "1          None          12209     Germany       58.919998  \n",
       "2          None          12209     Germany       88.379997  \n",
       "3          None          12209     Germany      149.399998  \n",
       "4          None          12209     Germany      173.339998  \n",
       "...         ...            ...         ...             ...  \n",
       "2150       None         01-012      Poland      391.880008  \n",
       "2151       None         01-012      Poland      412.190007  \n",
       "2152       None         01-012      Poland      432.500007  \n",
       "2153       None         01-012      Poland      452.810006  \n",
       "2154       None         01-012      Poland      461.530006  \n",
       "\n",
       "[2155 rows x 15 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_totals.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f429d5-1d58-48c0-b2b5-def947040e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cb8ad-7a44-47ac-ba6b-058e586b4edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9acd6e2e-6d3b-45e5-978e-50b9dd7a7284",
   "metadata": {},
   "source": [
    "**Window Function - Lag and Lead**\n",
    "\n",
    "`lag()` and `lead()` access previous and next rows within a partition. This enables comparison between consecutive orders for the same customer, useful for trend analysis and change detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "508cdae4-0b3d-45b6-91b1-cdd9ae43e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "comparison_window = Window.partitionBy(\"CustomerID\").orderBy(\"OrderDate\")\n",
    "order_comparison = orders.withColumn(\"PrevFreight\", lag(\"Freight\", 1).over(comparison_window))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43aafc-6963-4f49-8f89-be5b5624a0da",
   "metadata": {},
   "source": [
    "**Advanced Aggregation with Conditional Logic**\n",
    "\n",
    "Combines aggregation with conditional logic using `when()`. This counts total products and conditionally counts discontinued products, enabling calculation of percentages and business metrics within the same aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aa569-ed4c-44ef-9105-8d63422c2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_performance = products.groupBy(\"CategoryID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"TotalProducts\"),\n",
    "        count(when(col(\"Discontinued\") == 1, col(\"ProductID\"))).alias(\"DiscontinuedCount\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0f4c8-b96e-4d29-b4e4-c318209c7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "product_performance = products.groupBy(\"CategoryID\") \\\n",
    "    .agg(\n",
    "        count(\"ProductID\").alias(\"TotalProducts\"),\n",
    "        sum(when(col(\"Discontinued\") == 1, 1).otherwise(0)).alias(\"DiscontinuedCount\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c7271-5826-4f3d-b305-8faeac0e3579",
   "metadata": {},
   "source": [
    "**Window Function - Statistical Measures**\n",
    "\n",
    " Applies statistical functions across window partitions without collapsing rows (unlike `groupBy`). Each product retains its row while gaining category-level statistics, enabling comparison of individual items against group averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9cf2-799d-4059-a760-ff3494a4527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_window = Window.partitionBy(\"CategoryID\")\n",
    "product_stats = products.withColumn(\"AvgCategoryPrice\", avg(\"UnitPrice\").over(stats_window))\n",
    "product_stats.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa78d9-4be1-4256-9069-f01f3d69de88",
   "metadata": {},
   "source": [
    "## Data Types ad Handling Missing Data \n",
    "\n",
    "Spark DataFrames support a rich type system that mirrors SQL data types while providing strong schema enforcement and optimization capabilities. The core data types include\n",
    "- primitive types (IntegerType, StringType, FloatType, DoubleType, BooleanType, DateType, TimestampType),\n",
    "- decimal types (DecimalType for precise numeric calculations), and\n",
    "- complex types (ArrayType, MapType, StructType for nested data).\n",
    "\n",
    "Each column in a DataFrame has a defined data type that determines how Spark stores, processes, and optimizes operations on that data. Unlike Python's dynamic typing, Spark's static typing enables the Catalyst optimizer to generate efficient execution plans and catch type-related errors at compile time rather than runtime.\n",
    "\n",
    "The nullable property in Spark schemas plays a crucial role in data integrity and query optimization. When a column is marked as `nullable=False`, Spark guarantees that the column cannot contain null values, enabling aggressive optimizations like eliminating null checks in generated code.\n",
    "\n",
    "Conversely, `nullable=True` columns require null-safe operations and additional runtime checks. The `NullType` represents columns that contain only null values, which can occur during data loading or as intermediate results in transformations. \n",
    "\n",
    "Proper null handling is essential because null values propagate through most operations (e.g., `null + 5 = null`) and require explicit handling using functions like `isNull()`, `isNotNull()`, `coalesce()`, or `when().otherwise()` constructs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b0276-8bd1-4fc1-9d9b-1916ddafd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema definition with nullable control\n",
    "StructField(\"CustomerID\", StringType(), False)     # Cannot be null\n",
    "StructField(\"Region\", StringType(), True)          # Can be null\n",
    "\n",
    "# Null value analysis\n",
    "customers.select([count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") for c in customers.columns])\n",
    "\n",
    "# Null-safe operations\n",
    "when(col(\"Region\").isNull(), \"No Region\").otherwise(col(\"Region\"))\n",
    "\n",
    "# Type conversions\n",
    "col(\"UnitPrice\").cast(IntegerType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97196fbd-3cfc-4882-9933-fac96c525561",
   "metadata": {},
   "source": [
    "PySpark provides comprehensive tools for handling missing data through its DataFrame API and DataFrameNaFunctions class. Missing data in Spark is primarily represented as NULL values, which are distinct from empty strings, zeros, or NaN (Not a Number) values.\n",
    "\n",
    "Spark's approach to missing data is SQL-compliant, meaning NULL values propagate through most operations (e.g., NULL + 5 = NULL) and are excluded from aggregations by default. The framework offers both automatic handling (like ignoring NULLs in aggregations) and explicit control through functions like `isNull()`, `isNotNull()`, `na.drop()`, and `na.fill()`.\n",
    "\n",
    "The DataFrameNaFunctions class, accessed via df.na, provides the primary interface for missing data operations. Key strategies include\n",
    "- dropping rows with na.drop() (with options for thresholds and specific columns),\n",
    "- filling values with na.fill() (supporting different values per column), and\n",
    "- replacing values with na.replace().\n",
    "\n",
    "Advanced techniques involve conditional imputation using\n",
    "- `when().otherwise()` constructs,\n",
    "-  forward/backward filling with window functions, and\n",
    "-   statistical imputation using calculated means or medians.\n",
    "\n",
    "Spark also provides null-safe operations like `eqNullSafe()` for comparisons and coalesce() for selecting the first non-null value from multiple columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330b54c-f2c1-43c7-93e8-cf5d108445fd",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDFs) and User Defined Aggregate Functions (UDAFs)\n",
    "\n",
    "**User Defined Functions (UDFs)** in PySpark allow you to extend Spark's built-in function library with custom Python logic that can be applied to DataFrame columns. UDFs are essential when built-in functions cannot handle specific business logic, complex string processing, or domain-specific calculations. \n",
    "\n",
    "There are two main types:\n",
    "1. Standard UDFs that process one row at a time, and\n",
    "2. Pandas UDFs (vectorized UDFs) that leverage Apache Arrow for better performance by processing entire columns as pandas Series.\n",
    "\n",
    "UDFs are registered using the `udf()` function with a specified return type, and they can handle complex operations like regex processing, mathematical calculations, or external API calls.\n",
    "\n",
    "**User Defined Aggregate Functions (UDAFs)** enable custom aggregation logic across groups of rows, though PySpark doesn't have direct UDAF support like Scala Spark. Instead, you can achieve similar functionality by combining `collect_list()` with UDFs, or using `pandas_udf` with `groupby().apply()` for more complex aggregations. These are useful for implementing custom statistical measures, weighted calculations, or business-specific metrics that aren't available in Spark's standard aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfe3bc-0b85-4253-ae3d-3ca5e0b96ab3",
   "metadata": {},
   "source": [
    "**Standard UDF - Phone Number Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93541331-735b-447b-aa0c-4c3e8071bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def standardize_phone(phone_str):\n",
    "    if not phone_str:\n",
    "        return None\n",
    "    digits = re.sub(r'\\D', '', phone_str)\n",
    "    if len(digits) == 10:\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "    # ... complex formatting logic\n",
    "    \n",
    "standardize_phone_udf = udf(standardize_phone, StringType())\n",
    "customers.withColumn(\"StandardizedPhone\", standardize_phone_udf(col(\"Phone\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a70e6-98ef-4bbe-92fb-a5705b0d688f",
   "metadata": {},
   "source": [
    "**Custom Aggregation - Weighted Average**\n",
    "\n",
    "Performance Considerations: Pandas UDFs are significantly faster than standard UDFs due to vectorization and reduced serialization overhead. However, UDFs should be used judiciously as they break Spark's optimization capabilities and require data movement between JVM and Python processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c391dd9-4a73-4325-b99f-fe4cc5c717e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "def weighted_average_freight(freight_values, order_counts):\n",
    "    total_weighted = sum(f * c for f, c in zip(freight_values, order_counts) if f and c)\n",
    "    total_weights = sum(c for c in order_counts if c)\n",
    "    return total_weighted / total_weights if total_weights > 0 else 0.0\n",
    "\n",
    "orders.groupBy(\"ShipCountry\").agg(collect_list(\"Freight\").alias(\"freight_list\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fe31a-a7ad-4def-a37d-7a6c8a5b14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0c8b3-844d-464c-a456-505b5ae204da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4607db93-ef76-4a90-80f8-4561309a6e73",
   "metadata": {},
   "source": [
    "**Pandas UDF - Vectorized Z-Score Calculation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bf91e-6e9f-4062-af4b-b2620f0f5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, col\n",
    "# from pyspark.sql.types import StructType, StructField, FloatType\n",
    "# import pandas as pd\n",
    "\n",
    "# def calculate_zscore_group(pdf):\n",
    "#     \"\"\"Calculate Z-score within each group\"\"\"\n",
    "#     mean_price = pdf['UnitPrice'].mean()\n",
    "#     std_price = pdf['UnitPrice'].std()\n",
    "    \n",
    "#     if std_price > 0:\n",
    "#         pdf['PriceZScore'] = (pdf['UnitPrice'] - mean_price) / std_price\n",
    "#     else:\n",
    "#         pdf['PriceZScore'] = 0.0\n",
    "    \n",
    "#     return pdf2\n",
    "\n",
    "# # Create the output schema by adding the new column\n",
    "# # output_schema = StructType(products.schema.fields + [StructField(\"PriceZScore\", FloatType(), True)])\n",
    "# schema_str = \"ProductID int, ProductName string, SupplierID int, CategoryID int, QuantityPerUnit string, UnitPrice float, UnitsInStock int, UnitsOnOrder int, ReorderLevel int, Discontinued int, PriceZScore float\"\n",
    "\n",
    "# products_zscore = products.groupBy(\"CategoryID\").applyInPandas(\n",
    "#     calculate_zscore_group, \n",
    "#     schema=schema_str\n",
    "# )\n",
    "\n",
    "# # # Apply to grouped data\n",
    "# # products_zscore = products.groupBy(\"CategoryID\").applyInPandas(\n",
    "# #     calculate_zscore_group, \n",
    "# #     schema=output_schema\n",
    "# # )\n",
    "# products_zscore.printSchema()\n",
    "\n",
    "# #products_zscore.select(\"CategoryID\", \"ProductName\", \"UnitPrice\", \"PriceZScore\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468a0dc-b024-4d61-a798-e4bb03a6909a",
   "metadata": {},
   "source": [
    "Alternatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2084d-6161-464d-97ad-953c387d34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window\n",
    "window = Window.partitionBy(\"CategoryID\")\n",
    "\n",
    "# Calculate Z-score using window functions\n",
    "products_zscore = products \\\n",
    "    .withColumn(\"AvgPrice\", avg(\"UnitPrice\").over(window)) \\\n",
    "    .withColumn(\"StdPrice\", stddev(\"UnitPrice\").over(window)) \\\n",
    "    .withColumn(\"PriceZScore\", \n",
    "        (col(\"UnitPrice\") - col(\"AvgPrice\")) / col(\"StdPrice\")\n",
    "    )\n",
    "\n",
    "# Now you can select the column\n",
    "products_zscore.select(\"CategoryID\", \"ProductName\", \"UnitPrice\", \"PriceZScore\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076c7b-52ed-4cb6-ba64-6228ca2df79d",
   "metadata": {},
   "source": [
    "## Integrating with SQL: Using Spark SQL queries with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5faa5-70d8-44a7-8766-45647dcc7cef",
   "metadata": {},
   "source": [
    "Spark SQL provides a powerful way to integrate SQL queries directly into PySpark applications, allowing you to leverage familiar SQL syntax while maintaining the performance benefits of Spark's distributed computing. The integration works through temporary views that register DataFrames as SQL tables, enabling seamless switching between DataFrame API and SQL syntax within the same application. This approach is particularly valuable for teams with strong SQL backgrounds or when working with complex analytical queries that are more naturally expressed in SQL.\n",
    "\n",
    "The core integration mechanism involves registering DataFrames as temporary views using `createOrReplaceTempView()`, then executing SQL queries with `spark.sql()` which returns DataFrames that can be further processed using the DataFrame API. This bidirectional integration allows you to use SQL for complex joins, window functions, and analytical queries while leveraging Python's DataFrame API for data manipulation, machine learning pipelines, and custom transformations. Spark SQL supports the full SQL standard including CTEs, subqueries, window functions, and advanced analytical functions, all optimized by the same Catalyst optimizer that powers the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be1377-3aed-4976-af35-e5fa16e98e96",
   "metadata": {},
   "source": [
    "**Register DataFrames as Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f837d03-2a0c-4a2c-a621-9f7d93d08486",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.createOrReplaceTempView(\"customers\")\n",
    "orders.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09cc5-e754-4df2-861b-7059aa1f6368",
   "metadata": {},
   "source": [
    "**Execute SQL Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65bd3e07-2fbf-485d-a982-c3522003c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- OrderCount: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>OrderCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Save-a-lot Markets</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ernst Handel</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QUICK-Stop</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rattlesnake Canyon Grocery</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hungry Owl All-Night Grocers</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GROSELLA-Restaurante</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Lazy K Kountry Store</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Centro comercial Moctezuma</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Paris spécialités</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>FISSA Fabrica Inter. Salchichas S.A.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CompanyName  OrderCount\n",
       "0                     Save-a-lot Markets         116\n",
       "1                           Ernst Handel         102\n",
       "2                             QUICK-Stop          86\n",
       "3             Rattlesnake Canyon Grocery          71\n",
       "4           Hungry Owl All-Night Grocers          55\n",
       "..                                   ...         ...\n",
       "86                  GROSELLA-Restaurante           4\n",
       "87                  Lazy K Kountry Store           2\n",
       "88            Centro comercial Moctezuma           2\n",
       "89                     Paris spécialités           0\n",
       "90  FISSA Fabrica Inter. Salchichas S.A.           0\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT c.CompanyName, COUNT(o.OrderID) as OrderCount\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.CustomerID = o.CustomerID\n",
    "    GROUP BY c.CompanyName\n",
    "    ORDER BY OrderCount DESC\n",
    "\"\"\")\n",
    "result.printSchema()\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21eefd3-ddd7-4c0a-97be-120167a0e084",
   "metadata": {},
   "source": [
    "**Mix SQL and DataFrame API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec4923-d949-4d38-88a2-a4148778d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with SQL\n",
    "sql_result = spark.sql(\"SELECT * FROM customers WHERE Country = 'USA'\")\n",
    "# Continue with DataFrame API\n",
    "final_result = sql_result.filter(col(\"Region\").isNotNull()).orderBy(\"CompanyName\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ddd40-ae76-4a1a-b078-dc01c0071124",
   "metadata": {},
   "source": [
    "**Complex Analytics with CTEs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c91d64-bd93-45da-acda-1a1076fbb1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        SELECT YEAR(OrderDate) as year, MONTH(OrderDate) as month, SUM(Freight) as total\n",
    "        FROM orders GROUP BY YEAR(OrderDate), MONTH(OrderDate)\n",
    "    )\n",
    "    SELECT *, LAG(total) OVER (ORDER BY year, month) as prev_month\n",
    "    FROM monthly_sales\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf94981-892c-4d95-9a46-58f1ea2f0528",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ea51f-d40f-45f5-930d-8f649cad8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
