{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c3b86b-1ebc-45a3-8c13-9455050154b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load nbheader.py\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col as S\n",
    "from pyspark.sql import DataFrame, Row, Window\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92fc552-2571-4db0-ab81-aa6b6b9ec21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/08 17:02:15 WARN Utils: Your hostname, RCBM8368-DIII.local, resolves to a loopback address: 127.0.0.1; using 10.250.32.131 instead (on interface en0)\n",
      "25/10/08 17:02:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/pmolnar/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/pmolnar/.ivy2.5.2/jars\n",
      "io.graphframes#graphframes-spark4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-99d6603e-8e63-42f6-9bcd-d74f83443ef8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.graphframes#graphframes-spark4_2.13;0.9.2 in central\n",
      ":: resolution report :: resolve 58ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.graphframes#graphframes-spark4_2.13;0.9.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-99d6603e-8e63-42f6-9bcd-d74f83443ef8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "25/10/08 17:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.250.32.131:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1297c8bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 63540)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/pmolnar/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pmolnar/.base/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Intialise Spark session for GraphFrame, use equivalent of pyspark --packages io.graphframes:graphframes-spark4_2.13:0.9.2\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.graphframes:graphframes-spark4_2.13:0.9.2\") \\\n",
    "    .getOrCreate()\n",
    "# spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "spark.getActiveSession()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847a76cc-22c1-4b1c-8106-af77124da10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./tmp/graphframes-checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2b4135-0b83-42dc-8809-cdda8610a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"./tmp/graphframes-checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b355ea-386a-4b9d-9a84-7c9fd5754e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------------+\n",
      "| id|   name|          pagerank|\n",
      "+---+-------+------------------+\n",
      "|  a|  Alice| 1.313776242780802|\n",
      "|  c|Charlie|1.0007283492186585|\n",
      "|  d|  David|0.8427477040002699|\n",
      "|  b|    Bob|0.8427477040002699|\n",
      "+---+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install graphframes-py and GraphFrames jar if running in Colab or new environment\n",
    "# !pip install graphframes-py\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, lit\n",
    "\n",
    "\n",
    "# 1. Prepare vertices and edges DataFrames\n",
    "vertices = spark.createDataFrame([\n",
    "    (\"a\", \"Alice\"),\n",
    "    (\"b\", \"Bob\"),\n",
    "    (\"c\", \"Charlie\"),\n",
    "    (\"d\", \"David\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "edges = spark.createDataFrame([\n",
    "    (\"a\", \"b\", \"knows\"),\n",
    "    (\"b\", \"c\", \"knows\"),\n",
    "    (\"c\", \"a\", \"knows\"),\n",
    "    (\"a\", \"d\", \"knows\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# 2. Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "\n",
    "# 3. Run PageRank example\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=5)\n",
    "results.vertices.select(\"id\", \"name\", \"pagerank\").orderBy(S(\"pagerank\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0533153-7aea-40cc-8528-dbff9e39e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|   component|\n",
      "+---+------------+\n",
      "|  a|807453851648|\n",
      "|  b|807453851648|\n",
      "|  c|807453851648|\n",
      "|  d|807453851648|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 17:03:02 WARN ConnectedComponents$: The DataFrame returned by ConnectedComponents is persisted and loaded.\n"
     ]
    }
   ],
   "source": [
    "# 4. Find Connected Components\n",
    "## fix this: components = g.connectedComponents()\n",
    "components = g.connectedComponents()\n",
    "components.select(\"id\", \"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced8a02c-90bc-4c17-89d5-5afc28e5331c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  a|    1|\n",
      "|  b|    1|\n",
      "|  c|    1|\n",
      "|  d|    0|\n",
      "+---+-----+\n",
      "\n",
      "+---+---+---+---+\n",
      "|  a|  e|  b| e2|\n",
      "+---+---+---+---+\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Find triangle counts (clustering)\n",
    "triangles = g.triangleCount()\n",
    "triangles.select(\"id\", \"count\").show()\n",
    "\n",
    "# 6. Motif finding: look for mutual relationships\n",
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "motifs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e99cc8-dd8e-4549-9bf5-10371fd14b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------------+-------------+\n",
      "|              u1|           e1|              u2|           e2|\n",
      "+----------------+-------------+----------------+-------------+\n",
      "|    {b, Bob, 28}|{b, a, likes}|  {a, Alice, 34}|{a, b, likes}|\n",
      "|{c, Charlie, 31}|{c, a, likes}|  {a, Alice, 34}|{a, c, likes}|\n",
      "|  {a, Alice, 34}|{a, b, likes}|    {b, Bob, 28}|{b, a, likes}|\n",
      "|{c, Charlie, 31}|{c, b, likes}|    {b, Bob, 28}|{b, c, likes}|\n",
      "|  {a, Alice, 34}|{a, c, likes}|{c, Charlie, 31}|{c, a, likes}|\n",
      "|    {b, Bob, 28}|{b, c, likes}|{c, Charlie, 31}|{c, b, likes}|\n",
      "+----------------+-------------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create vertices DataFrame: user id, name, and age\n",
    "vertices = spark.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 28),\n",
    "    (\"c\", \"Charlie\", 31),\n",
    "    (\"d\", \"David\", 22)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Create edges DataFrame: who liked whose post\n",
    "edges = spark.createDataFrame([\n",
    "    (\"a\", \"b\", \"likes\"),\n",
    "    (\"b\", \"a\", \"likes\"),\n",
    "    (\"a\", \"c\", \"likes\"),\n",
    "    (\"c\", \"a\", \"likes\"),\n",
    "    (\"b\", \"c\", \"likes\"),\n",
    "    (\"c\", \"b\", \"likes\"),\n",
    "    (\"d\", \"a\", \"likes\")  # David likes Alice only (non-mutual)\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Build the graph\n",
    "g = GraphFrame(vertices, edges)\n",
    "\n",
    "# Motif query: look for mutual 'likes' relationships\n",
    "motifs = g.find(\"(u1)-[e1]->(u2); (u2)-[e2]->(u1)\")\n",
    "\n",
    "# Filter for cases where either user is older than 30\n",
    "filtered = motifs.filter(\"u1.age > 30 or u2.age > 30\")\n",
    "\n",
    "filtered.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1747b958-8a8c-4634-a6c8-35e5859b92bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u1: struct (nullable = false)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: long (nullable = true)\n",
      " |-- e1: struct (nullable = false)\n",
      " |    |-- src: string (nullable = true)\n",
      " |    |-- dst: string (nullable = true)\n",
      " |    |-- relationship: string (nullable = true)\n",
      " |-- u2: struct (nullable = false)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: long (nullable = true)\n",
      " |-- e2: struct (nullable = false)\n",
      " |    |-- src: string (nullable = true)\n",
      " |    |-- dst: string (nullable = true)\n",
      " |    |-- relationship: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baeb2385-efe9-4702-892a-a63989a326b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+---+\n",
      "|   name|age|   name|age|\n",
      "+-------+---+-------+---+\n",
      "|    Bob| 28|  Alice| 34|\n",
      "|Charlie| 31|  Alice| 34|\n",
      "|  Alice| 34|    Bob| 28|\n",
      "|Charlie| 31|    Bob| 28|\n",
      "|  Alice| 34|Charlie| 31|\n",
      "|    Bob| 28|Charlie| 31|\n",
      "+-------+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.select(\"u1.name\", \"u1.age\", \"u2.name\", \"u2.age\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541f851d-4cb5-4c33-a803-57b6e1a31910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run BFS example\n",
    "paths = g.bfs(\"name = 'Alice'\", \"name = 'Charlie'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fe61d7-84b5-4a9f-a559-03747e26a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from: struct (nullable = false)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: long (nullable = true)\n",
      " |-- e0: struct (nullable = false)\n",
      " |    |-- src: string (nullable = true)\n",
      " |    |-- dst: string (nullable = true)\n",
      " |    |-- relationship: string (nullable = true)\n",
      " |-- to: struct (nullable = false)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paths.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4eb600-113a-42c8-9d81-0961c08c5e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>e0</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(a, Alice, 34)</td>\n",
       "      <td>(a, c, likes)</td>\n",
       "      <td>(c, Charlie, 31)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             from             e0                to\n",
       "0  (a, Alice, 34)  (a, c, likes)  (c, Charlie, 31)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "287ac72e-33d2-4c19-97cd-ac89add81eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
